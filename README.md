<div align="center">
<img src="./assets/minicpm_logo.png" width="500em" ></img> 
</div>

<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> | <a href="https://github.com/OpenBMB/MiniCPM/blob/main/README-en.md">English</a>
    <p>
</h4>


<p align="center">
<a href="https://openbmb.vercel.app/?category=Chinese+Blog" target="_blank">MiniCPM æŠ€æœ¯åšå®¢</a> |
<a href="https://modelbest.feishu.cn/wiki/D2tFw8Pcsi5CIzkaHNacLK64npg" target="_blank">MiniCPM çŸ¥è¯†åº“</a> |
<a href="https://arxiv.org/abs/2404.06395" target="_blank">MiniCPM è®ºæ–‡</a> |
<a href="https://github.com/OpenBMB/MiniCPM-V/" target="_blank">MiniCPM-V ä»“åº“</a> |
åŠ å…¥æˆ‘ä»¬çš„ <a href="https://discord.gg/3cGQn9b3YM" target="_blank">discord</a> å’Œ <a href="https://github.com/OpenBMB/MiniCPM/blob/main/assets/wechat.jpg" target="_blank">å¾®ä¿¡ç¾¤</a>
 
</p>

## æ›´æ–°æ—¥å¿—ğŸ”¥

- [2024.09.28] **[LLMxMapReduce](https://github.com/thunlp/LLMxMapReduce) å¼€æºï¼Œæ”¯æŒMiniCPM3-4Bï¼Œç†è®ºä¸Šæ”¯æŒæ— é™é•¿æ–‡æœ¬è¾“å…¥ï¼**
- [2024.09.18] **[SGLang](https://github.com/sgl-project/sglang) å·²ç»æ”¯æŒ MiniCPM3-4B (æ¨èä½¿ç”¨)ï¼ç”±äº SGLang v0.3 å¯¹ MiniCPM3 ä¸­ä½¿ç”¨çš„ MLA ç»“æ„è¿›è¡Œäº†æ¨ç†ä¼˜åŒ–ï¼Œååé‡ç›¸æ¯”äº vLLM æé«˜ 70%ï¼**[[ç”¨æ³•](#sglangæ¨è)]
- [2024.09.16] [llama.cpp](https://github.com/ggerganov/llama.cpp/releases/tag/b3765) å·²ç»å®˜æ–¹æ”¯æŒ MiniCPM3-4Bï¼[[GGUFæ¨¡å‹](https://huggingface.co/openbmb/MiniCPM3-4B-GGUF)|[ç”¨æ³•](#llamacpp)]
- [2024.09.05] å‘å¸ƒ [**MiniCPM3-4B**](https://huggingface.co/openbmb/MiniCPM3-4B)ï¼è¯¥æ¨¡å‹çš„è¡¨ç°è¶…è¶Š Phi-3.5-mini-instruct å’Œ GPT-3.5-Turbo-0125ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¯”è‚© Llama3.1-8B-Instructã€Qwen2-7B-Instructã€GLM-4-9B-Chat ç­‰å¤šä¸ª 7B-9B å‚æ•°é‡çš„æ¨¡å‹ã€‚
- [2024.07.09] MiniCPM-2B å·²ç»æ”¯æŒä½¿ç”¨ [SGLang](#sglang-æ¨ç†) æ¨ç†ï¼
- [2024.07.05] å‘å¸ƒ [MiniCPM-S-1B](https://huggingface.co/openbmb/MiniCPM-S-1B-sft)ï¼è¯¥æ¨¡å‹åœ¨ä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ— æŸçš„å‰æä¸‹ï¼ŒFFN å±‚å®ç°äº† 87.89% çš„å¹³å‡ç¨€ç–åº¦ï¼Œå°† FFN FLOPs é™ä½äº† 84%ã€‚
- [2024.04.11] å‘å¸ƒ [MiniCPM-2B-128k](https://huggingface.co/openbmb/MiniCPM-2B-128k)ã€[MiniCPM-MoE-8x2B](https://huggingface.co/openbmb/MiniCPM-MoE-8x2B) å’Œ [MiniCPM-1B](https://huggingface.co/openbmb/MiniCPM-1B-sft-bf16)ï¼ç‚¹å‡»[è¿™é‡Œ](https://openbmb.vercel.app/?category=Chinese+Blog)æŸ¥çœ‹æŠ€æœ¯åšå®¢ã€‚
- [2024.03.16] MiniCPM-2B çš„ 30 ä½™ä¸ªä¸­é—´æ£€æŸ¥ç‚¹å¼€æ”¾äº†ï¼[HuggingFaceé“¾æ¥](https://huggingface.co/openbmb/MiniCPM-2B-history)
- [2024.02.01] å‘å¸ƒ [**MiniCPM-2B**](https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16)ï¼è¯¥æ¨¡å‹åœ¨å…¬å¼€è¯„æµ‹é›†ä¸Šä¸ Mistral-7B è¡¨ç°ç›¸è¿‘ï¼ˆä¸­æ–‡ã€æ•°å­¦ã€ä»£ç èƒ½åŠ›æ›´ä¼˜ï¼‰ï¼Œæ•´ä½“æ€§èƒ½è¶…è¶Š Llama2-13Bã€MPT-30Bã€Falcon-40B ç­‰æ¨¡å‹ã€‚

## ç›®å½•

- [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
- [MiniCPM 3.0](#minicpm-30)
  - [è¯„æµ‹ç»“æœ](#è¯„æµ‹ç»“æœ)
    - [ç»¼åˆè¯„æµ‹](#ç»¼åˆè¯„æµ‹)
    - [å·¥å…·è°ƒç”¨èƒ½åŠ›](#å·¥å…·è°ƒç”¨èƒ½åŠ›)
    - [é•¿æ–‡æœ¬èƒ½åŠ›](#é•¿æ–‡æœ¬èƒ½åŠ›)
  - [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
    - [HuggingFace](#huggingface)
    - [vLLM](#vllm)
    - [llama.cpp](#llamacpp)
  - [æ¨¡å‹å¾®è°ƒ](#æ¨¡å‹å¾®è°ƒ)
    - [LLaMA-Factory](#llama-factory)
  - [è¿›é˜¶åŠŸèƒ½](#è¿›é˜¶åŠŸèƒ½)
    - [å·¥å…·è°ƒç”¨](#å·¥å…·è°ƒç”¨)
    - [ä»£ç è§£é‡Šå™¨](#ä»£ç è§£é‡Šå™¨)
- [MiniCPM 2.0](#minicpm-20)
- [MiniCPM 1.0](#minicpm-10)


## æ¨¡å‹ä¸‹è½½
 
  | HuggingFace | ModelScope |
  |-------------|------------|
  |[MiniCPM3-4B](https://huggingface.co/openbmb/MiniCPM3-4B)|[MiniCPM3-4B](https://www.modelscope.cn/models/OpenBMB/MiniCPM3-4B)|
  |[MiniCPM-2B-sft](https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16)|[MiniCPM-2B-sft](https://modelscope.cn/models/OpenBMB/miniCPM-bf16)|
  |[MiniCPM-2B-dpo](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16)|[MiniCPM-2B-dpo](https://modelscope.cn/models/OpenBMB/MiniCPM-2B-dpo-bf16/summary)|
  |[MiniCPM-2B-128k](https://huggingface.co/openbmb/MiniCPM-2B-128k) |[MiniCPM-2B-128k](https://modelscope.cn/models/openbmb/MiniCPM-2B-128k/summary)| 
  |[MiniCPM-MoE-8x2B](https://huggingface.co/openbmb/MiniCPM-MoE-8x2B) |[MiniCPM-MoE-8x2B](https://modelscope.cn/models/OpenBMB/MiniCPM-MoE-8x2B)| 
  |[MiniCPM-1B](https://huggingface.co/openbmb/MiniCPM-1B-sft-bf16) | [MiniCPM-1B](https://modelscope.cn/models/OpenBMB/MiniCPM-1B-sft-bf16) |
  |[MiniCPM-S-1B](https://huggingface.co/openbmb/MiniCPM-S-1B-sft)|[MiniCPM-S-1B](https://modelscope.cn/models/OpenBMB/MiniCPM-S-1B-sft)|

  æ³¨: æ›´å¤šæ¨¡å‹ç‰ˆæœ¬è§[è¿™é‡Œ](https://huggingface.co/collections/openbmb/minicpm-2b-65d48bf958302b9fd25b698f)ã€‚


## MiniCPM 3.0

MiniCPM 3.0 æ˜¯ä¸€ä¸ª 4B å‚æ•°é‡çš„è¯­è¨€æ¨¡å‹ï¼Œç›¸æ¯” MiniCPM1.0/2.0ï¼ŒåŠŸèƒ½æ›´åŠ å…¨é¢ï¼Œç»¼åˆèƒ½åŠ›å¤§å¹…æå‡ï¼Œå¤šæ•°è¯„æµ‹é›†ä¸Šçš„æ•ˆæœæ¯”è‚©ç”šè‡³è¶…è¶Šä¼—å¤š 7B-9B æ¨¡å‹ã€‚
* **æ”¯æŒå·¥å…·è°ƒç”¨ğŸ› ï¸ï¼ˆFunction Callingï¼‰å’Œä»£ç è§£é‡Šå™¨ğŸ’»ï¼ˆCode Interpreterï¼‰**ï¼š[Berkeley Function Calling Leaderboard (BFCL)](https://gorilla.cs.berkeley.edu/leaderboard.html) ä¸Šå–å¾— 9B è§„æ¨¡ä»¥ä¸‹ SOTAï¼Œè¶…è¶Š GLM-4-9B-Chatã€Qwen2-7B-Instructã€‚
* **è¶…å¼ºçš„æ¨ç†èƒ½åŠ›ğŸ§®**ï¼šæ•°å­¦èƒ½åŠ›æ–¹é¢ï¼Œ[MathBench](https://open-compass.github.io/MathBench/) ä¸Šçš„æ•ˆæœè¶…è¶Š GPT-3.5-Turbo ä»¥åŠå¤šä¸ª 7B-9B æ¨¡å‹ã€‚åœ¨éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ [LiveCodeBench](https://livecodebench.github.io/) ä¸Šï¼Œæ•ˆæœè¶…è¶Š Llama3.1-8B-Instructã€‚
* **å‡ºè‰²çš„ä¸­è‹±æ–‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›ğŸ¤–**ï¼šè‹±æ–‡æŒ‡ä»¤éµå¾ª [IFEval](https://huggingface.co/datasets/google/IFEval)ã€ä¸­æ–‡æŒ‡ä»¤éµå¾ª [FollowBench-zh](https://huggingface.co/datasets/YuxinJiang/FollowBench) æ•ˆæœè¶…è¶Š GLM-4-9B-Chatã€Qwen2-7B-Instructã€‚
* **é•¿æ–‡æœ¬èƒ½åŠ›**ï¼šåŸç”Ÿæ”¯æŒ 32k ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ32k é•¿åº¦å†…å¤§æµ·æé’ˆå…¨ç»¿ã€‚æå‡º [LLMxMapReduce](https://github.com/thunlp/LLMxMapReduce) ï¼Œç†è®ºå¯å¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦è¾¾åˆ° +âˆï¼Œåœ¨ç»¼åˆæ€§é•¿æ–‡æœ¬è¯„æµ‹åŸºå‡† [InfiniteBench](https://github.com/OpenBMB/InfiniteBench) å¹³å‡å¾—åˆ†è¶…è¶ŠGPT-4ã€KimiChatç­‰æ ‡æ†æ¨¡å‹ã€‚
* **RAGèƒ½åŠ›**ï¼šæˆ‘ä»¬å‘å¸ƒäº† [MiniCPM RAG å¥—ä»¶](https://huggingface.co/collections/openbmb/minicpm-rag-suite-66d976b4204cd0a4f8beaabb)ã€‚åŸºäº MiniCPM ç³»åˆ—æ¨¡å‹çš„ [MiniCPM-Embedding](https://huggingface.co/openbmb/MiniCPM-Embedding)ã€[MiniCPM-Reranker](https://huggingface.co/openbmb/MiniCPM-Reranker) åœ¨ä¸­æ–‡ã€ä¸­è‹±è·¨è¯­è¨€æ£€ç´¢æµ‹è¯•ä¸­å–å¾— SOTA è¡¨ç°ï¼›é’ˆå¯¹ RAG åœºæ™¯çš„ [MiniCPM3-RAG-LoRA](https://huggingface.co/openbmb/MiniCPM3-RAG-LoRA) åœ¨å¼€æ”¾åŸŸé—®ç­”ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šè¶…è¶Š Llama3-8Bã€Baichuan2-13B ç­‰æ¨¡å‹ã€‚

### è¯„æµ‹ç»“æœ

#### ç»¼åˆè¯„æµ‹

<table>
    <tr>
        <td>è¯„æµ‹é›†</td>
        <td>Qwen2-7B-Instruct</td>
        <td>GLM-4-9B-Chat</td>
        <td>Gemma2-9B-it</td>
        <td>Llama3.1-8B-Instruct</td>
        <td>GPT-3.5-Turbo-0125</td>
        <td>Phi-3.5-mini-Instruct(3.8B)</td>
        <td>MiniCPM3-4B </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>è‹±æ–‡èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>MMLU</td>
        <td>70.5</td>
        <td>72.4</td>
        <td>72.6</td>
        <td>69.4</td>
        <td>69.2</td>
        <td>68.4</td>
        <td>67.2 </td>
    </tr>
    <tr>
        <td>BBH</td>
        <td>64.9</td>
        <td>76.3</td>
        <td>65.2</td>
        <td>67.8</td>
        <td>70.3</td>
        <td>68.6</td>
        <td>70.2 </td>
    </tr>
    <tr>
        <td>MT-Bench</td>
        <td>8.41</td>
        <td>8.35</td>
        <td>7.88</td>
        <td>8.28</td>
        <td>8.17</td>
        <td>8.60</td>
        <td>8.41 </td>
    </tr>
    <tr>
        <td>IFEVAL (Prompt Strict-Acc.)</td>
        <td>51.0</td>
        <td>64.5</td>
        <td>71.9</td>
        <td>71.5</td>
        <td>58.8</td>
        <td>49.4</td>
        <td>68.4 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>ä¸­æ–‡èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>CMMLU</td>
        <td>80.9</td>
        <td>71.5</td>
        <td>59.5</td>
        <td>55.8</td>
        <td>54.5</td>
        <td>46.9</td>
        <td>73.3 </td>
    </tr>
    <tr>
        <td>CEVAL</td>
        <td>77.2</td>
        <td>75.6</td>
        <td>56.7</td>
        <td>55.2</td>
        <td>52.8</td>
        <td>46.1</td>
        <td>73.6 </td>
    </tr>
    <tr>
        <td>AlignBench v1.1</td>
        <td>7.10</td>
        <td>6.61</td>
        <td>7.10</td>
        <td>5.68</td>
        <td>5.82</td>
        <td>5.73</td>
        <td>6.74 </td>
    </tr>
    <tr>
        <td>FollowBench-zh (SSR)</td>
        <td>63.0</td>
        <td>56.4</td>
        <td>57.0</td>
        <td>50.6</td>
        <td>64.6</td>
        <td>58.1</td>
        <td>66.8 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>æ•°å­¦èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>MATH</td>
        <td>49.6</td>
        <td>50.6</td>
        <td>46.0</td>
        <td>51.9</td>
        <td>41.8</td>
        <td>46.4</td>
        <td>46.6 </td>
    </tr>
    <tr>
        <td>GSM8K</td>
        <td>82.3</td>
        <td>79.6</td>
        <td>79.7</td>
        <td>84.5</td>
        <td>76.4</td>
        <td>82.7</td>
        <td>81.1 </td>
    </tr>
    <tr>
        <td>MathBench</td>
        <td>63.4</td>
        <td>59.4</td>
        <td>45.8</td>
        <td>54.3</td>
        <td>48.9</td>
        <td>54.9</td>
        <td>65.6 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>ä»£ç èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>HumanEval+</td>
        <td>70.1</td>
        <td>67.1</td>
        <td>61.6</td>
        <td>62.8</td>
        <td>66.5</td>
        <td>68.9</td>
        <td>68.3 </td>
    </tr>
    <tr>
        <td>MBPP+</td>
        <td>57.1</td>
        <td>62.2</td>
        <td>64.3</td>
        <td>55.3</td>
        <td>71.4</td>
        <td>55.8</td>
        <td>63.2 </td>
    </tr>
    <tr>
        <td>LiveCodeBench v3</td>
        <td>22.2</td>
        <td>20.2</td>
        <td>19.2</td>
        <td>20.4</td>
        <td>24.0</td>
        <td>19.6</td>
        <td>22.6 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>å·¥å…·è°ƒç”¨èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>BFCL v2</td>
        <td>71.6</td>
        <td>70.1</td>
        <td>19.2</td>
        <td>73.3</td>
        <td>75.4</td>
        <td>48.4</td>
        <td>76.0 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>ç»¼åˆèƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>å¹³å‡åˆ†</td>
        <td>65.3</td>
        <td>65.0</td>
        <td>57.9</td>
        <td>60.8</td>
        <td>61.0</td>
        <td>57.2</td>
        <td><strong>66.3</strong></td>
    </tr>
</table>

#### å·¥å…·è°ƒç”¨èƒ½åŠ›

æˆ‘ä»¬åœ¨ [Berkeley Function Calling Leaderboard (BFCL)](https://gorilla.cs.berkeley.edu/leaderboard.html) ä¸Šæµ‹è¯•äº†æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼ŒMiniCPM3-4B åœ¨è¯¥æ¦œå•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†å¤šä¸ª 7B-9B å‚æ•°é‡çš„æ¨¡å‹ï¼Œä¼˜äº GPT-3.5-Turbo-0125ã€‚

<table>
    <tr>
        <td>æ¨¡å‹</td>
        <td>æ€»ä½“å‡†ç¡®ç‡</td>
        <td>AST Summary</td>
        <td>Exec Summary</td>
        <td>Irrelevance Detection</td>
        <td>Relevance Detection </td>
    </tr>
    <tr>
        <td>MiniCPM3-4B</td>
        <td>76.03%</td>
        <td>68.55%</td>
        <td>85.54%</td>
        <td>53.71%</td>
        <td>90.24% </td>
    </tr>
    <tr>
        <td>Llama3.1-8B-Instruct</td>
        <td>73.28%</td>
        <td>64.61%</td>
        <td>86.48%</td>
        <td>43.12%</td>
        <td>85.37% </td>
    </tr>
    <tr>
        <td>Qwen2-7B-Instruct</td>
        <td>71.61%</td>
        <td>65.71%</td>
        <td>79.57%</td>
        <td>44.70%</td>
        <td>90.24% </td>
    </tr>
    <tr>
        <td>GLM-4-9B-Chat</td>
        <td>70.08%</td>
        <td>60.69%</td>
        <td>80.02%</td>
        <td>55.02%</td>
        <td>82.93% </td>
    </tr>
    <tr>
        <td>Phi-3.5-mini-instruct</td>
        <td>48.44%</td>
        <td>38.89%</td>
        <td>54.04%</td>
        <td>46.78%</td>
        <td>65.85% </td>
    </tr>
    <tr>
        <td>Gemma2-9B-it</td>
        <td>19.18%</td>
        <td>5.41%</td>
        <td>18.50%</td>
        <td>88.88%</td>
        <td>7.32%</td>
    </tr>
</table>

#### é•¿æ–‡æœ¬èƒ½åŠ›

åœ¨ 32k çš„ä¸Šä¸‹æ–‡é•¿åº¦è¿›è¡Œ[å¤§æµ·æé’ˆ](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)æµ‹è¯•ï¼Œç»“æœå¦‚ä¸‹å›¾ï¼š

![needle](assets/eval_needle.jpeg)

åŒæ—¶æˆ‘ä»¬æå‡º[LLMxMapReduce](https://github.com/thunlp/LLMxMapReduce)ï¼Œåˆ©ç”¨åˆ†æ²»çš„ç­–ç•¥ï¼Œç†è®ºä¸Šå¯ä»¥å¤„ç†æ— é™é•¿åº¦çš„æ–‡æœ¬ã€‚æˆ‘ä»¬åœ¨[InfiniteBench](https://github.com/OpenBMB/InfiniteBench)ä¸Šæµ‹è¯•äº†æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œåœ¨LLMxMapReduceæ¡†æ¶çš„åŠ æŒä¸‹ï¼ŒMiniCPM3-4Båœ¨è¿™ä¸ªæ¦œå•çš„å¹³å‡å¾—åˆ†èƒ½å¤Ÿè¶…è¶Š GPT-4ã€KimiChat ç­‰æ ‡æ†æ¨¡å‹ã€‚

|                               | Context length| Qwen2-70b | Kimi-Chat(2024.06) | GPT-4 (From InfiniteBench) | MiniCPM 3.0 x MR | Qwen2-70b x MR | Llama3-70bx MR |
| ----------------------------- | ---------- | --------- | ------------------ | -------------------------- | --------------- | ------------ | ------------- |
| Math.Find                     | 87.9k      | 59.71%    | 18.57%             | 60.00%                     | 83.43%          | 54.29%       | **91.43%**        |
| Retrieve.KV                   | 89.9k      | 29.00%    | 69.20%             | 89.00%                     | 93.80%          | 98.80%       | **98.89%**        |
| En.Dia                        | 103.6K     | 23.00%    | 23.00%             | 7.50%                      | 12.50%          | **46.50%**       | 17.50%        |
| Code.Debug                    | 114.7k     | 45.43%    | 38.32%             | 54.31%                     | 25.63%          | 54.82%       | **62.94%**       |
| Retrieve.Number               | 122.4k     | **100.00%**  | 97.45%             | **100.00%**                   | 99.32%          | **100.00%**     | 99.79%        |
| Retrieve.PassKey              | 122.4k     | **100.00%**   | 99.32%             | **100.00%**                   | 98.81%          | **100.00%**     | **100.00%**      |
| En.Sum                        | 171.5K     | 31.85%    | 29.94%             | 14.73%                     | 25.89%          | **32.39%**       | 30.63%        |
| En.MC                         | 184.4k     | 81.66%    | 79.91%             | 68.12%                     | 66.38%          |**83.84%**      | 82.10%        |
| En.QA        | 192.6k     | 21.97%    | 18.80%             | 22.44%                     | 28.39%          | 23.13%       | **34.70%**      |
| Zh.QA        | 2068.6k    | 21.40%    | 19.84%             | **25.96%**                    | 23.66%          | 19.10%       | N/A           |
| avg w/o Zh.QA | /          | 51.92%    | 52.96%             | 55.33%                     | 59.29%          | 64.98%       | **68.64%**        |
| avg                           | /          | 48.86%    | 49.65%             | 52.39%                     | 55.55%          | **60.39%**       | N/A           |

### æ¨¡å‹æ¨ç†

#### Huggingface
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(0)

path = 'openbmb/MiniCPM3-4B'
tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)

responds, history = model.chat(tokenizer, "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼Œè¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•å’Œéšæ‚£ã€‚", temperature=0.7, top_p=0.7)
print(responds)
```

#### SGLangï¼ˆæ¨èï¼‰
* å®‰è£…

å‚è€ƒ SGLang [å®˜æ–¹ä»“åº“](ttps://github.com/sgl-project/sglang)ï¼Œé€šè¿‡*æºç *å®‰è£…æœ€æ–°ç‰ˆæœ¬ã€‚

* å¯åŠ¨æ¨ç†æœåŠ¡
```shell
python -m sglang.launch_server --model openbmb/MiniCPM3-4B --trust-remote-code --port 30000 --chat-template chatml
```

* ä½¿ç”¨ç¤ºä¾‹
```python
from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint

@function
def multi_turn_question(s, question_1, question_2):
    s += user(question_1)
    s += assistant(gen("answer_1", max_tokens=1024))
    s += user(question_2)
    s += assistant(gen("answer_2", max_tokens=1024))

set_default_backend(RuntimeEndpoint("http://localhost:30000"))

state = multi_turn_question.run(
    question_1="ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    question_2="å†™ä¸€ç¯‡å…³äºå®ƒçš„æ–‡ç« ",
)

for m in state.messages():
    print(m["role"], ":", m["content"])
```

#### vLLM
* å®‰è£… vllm
  ```shell
  pip install "vllm>=0.6.2"
  ```
* æ¨ç†
  ```python
  from transformers import AutoTokenizer
  from vllm import LLM, SamplingParams

  model_name = "openbmb/MiniCPM3-4B"
  prompt = [{"role": "user", "content": "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼Œè¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•å’Œéšæ‚£ã€‚"}]

  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  input_text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)

  llm = LLM(model=model_name,
      trust_remote_code=True,
      tensor_parallel_size=1
  )
  sampling_params = SamplingParams(top_p=0.7, temperature=0.7, max_tokens=1024)

  outputs = llm.generate(prompts=input_text, sampling_params=sampling_params)

  print(outputs[0].outputs[0].text)
  ```

#### llama.cpp

æˆ‘ä»¬æä¾›äº† MiniCPM3 çš„ [GGUF ç‰ˆæœ¬](https://huggingface.co/openbmb/MiniCPM3-4B-GGUF)ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ llama.cpp æ¨ç†ã€‚

* å®‰è£… llama.cpp
  ```shell
    git clone https://github.com/ggerganov/llama.cpp
    cd llama.cpp
    make 
  ```
* æ¨ç†
  ```shell
  ./llama-cli -c 1024 -m minicpm3-4b-fp16.gguf -n 1024 --top-p 0.7 --temp 0.7 --prompt "<|im_start|>user\nè¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼Œè¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•å’Œéšæ‚£ã€‚<|im_end|>\n<|im_start|>assistant\n"
  ```

### æ¨¡å‹å¾®è°ƒ
#### LLaMA-Factory
ç›®å‰æ¨¡å‹å¾®è°ƒæ”¯æŒ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)ï¼Œä½¿ç”¨æ–¹æ³•å‚è€ƒ [LLaMA-Factory å¾®è°ƒ](https://modelbest.feishu.cn/docx/Z7USdW4lloZzkZxQ14icJ3senjb?from=from_copylink)ã€‚

### è¿›é˜¶åŠŸèƒ½

å¯¹äºä»¥ä¸‹è¿›é˜¶åŠŸèƒ½ï¼Œæˆ‘ä»¬çš„æ ·ä¾‹ä»£ç ä¸­ä½¿ç”¨ [vLLM](#vllm) è¿›è¡Œæ¨ç†ã€‚

#### å·¥å…·è°ƒç”¨

æˆ‘ä»¬æä¾›äº†ä½¿ç”¨ MiniCPM3 è°ƒç”¨å·¥å…·çš„ç¤ºä¾‹ä»£ç ï¼š

```bash
cd demo/minicpm3/function_call
python function_call.py
```

å¦‚æœä½ æƒ³å¯åŠ¨ä¸€ä¸ªèƒ½å¤Ÿè°ƒç”¨å·¥å…·çš„æ¨ç†æœåŠ¡ï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```bash
cd demo/minicpm3/function_call
pip install -r requirements.txt
python openai_api_server.py \
    --model openbmb/MiniCPM3-4B \
    --served-model-name MiniCPM3-4B \
    --chat-template chatml.jinja \
    --dtype auto \
    --api-key token-abc123 \
    --tensor-parallel-size 1 \
    --trust-remote-code
```

ä¸‹é¢æ˜¯ä¸€ä¸ªè°ƒç”¨æœç´¢å·¥å…·å›ç­”é—®é¢˜çš„æ¼”ç¤ºï¼š

![function_call](./assets/function_call.gif)

#### ä»£ç è§£é‡Šå™¨

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª MiniCPM3 ä½¿ç”¨ä»£ç è§£é‡Šå™¨çš„ç¤ºä¾‹ä»£ç ï¼š

```bash
cd demo/minicpm3/code_interpreter
pip install -r requirements.txt
python code_interpreter.py openbmb/MiniCPM3-4B
```

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ä»£ç è§£é‡Šå™¨ç”ŸæˆäºŒç»´ç çš„æ¼”ç¤ºï¼š

![code_interpreter](./assets/code_interpreter.gif)

## MiniCPM 2.0

<details>
<summary>æŸ¥çœ‹ MiniCPM 2.0 çš„è¯¦ç»†ä¿¡æ¯</summary>

MiniCPM 2.0 ç³»åˆ—æ¨¡å‹å¯¹ MiniCPM è¿›è¡Œäº†å¤šä¸ªç»´åº¦çš„å‡çº§ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ¨¡å‹ç‰ˆæœ¬ï¼š
- MiniCPM-2B-128kï¼šå°† MiniCPM-2B çš„ä¸Šä¸‹æ–‡é•¿åº¦ä» 4k æ‰©å±•è‡³ 128kï¼Œåœ¨ InfiniteBench æµ‹è¯•é›†ä¸Šä¼˜äº ChatGLM3-6B-128kã€Yi-6B-200k ç­‰æ›´å¤§å‚æ•°é‡çš„æ¨¡å‹ã€‚
- MiniCPM-MoE-8x2Bï¼šåŸºäº MiniCPM-2B è¿›è¡Œ MoE æ‰©å±•ï¼Œç»¼åˆè¡¨ç°ç›¸æ¯”äº MiniCPM-2B å¹³å‡æé«˜ 4.5 ä¸ªç™¾åˆ†ç‚¹ã€‚
- MiniCPM-1Bï¼šç›¸æ¯”äº MiniCPM-2B æˆæœ¬ä¸‹é™ 60%ï¼Œç»¼åˆè¡¨ç°ä»ç„¶ä¼˜äº LLaMA2-13Bã€‚
- MiniCPM-S-1Bï¼šåœ¨ä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ— æŸçš„å‰æä¸‹ï¼ŒFFN å±‚å®ç°äº† 87.89% çš„å¹³å‡ç¨€ç–åº¦ï¼Œå°† FFN FLOPs é™ä½äº† 84%ã€‚ç»“åˆ PowerInfer æ¨ç†æ¡†æ¶ï¼Œè§£ç é€Ÿåº¦æå‡çº¦ 2.8 å€ã€‚

### è¯„æµ‹ç»“æœ

#### MiniCPM-2B-128k æ¨¡å‹è¯„æµ‹
| Model                               | avg   | avg w/o code&math | passkey | number_string | kv_retrieval | longbook_choice_eng | longbook_qa_chn | longbook_qa_eng | longbook_sum_eng | longdialogue_qa_eng | math_calc | math_find | code_debug | code_run |
|-------------------------------------|-------|-------------------|---------|---------------|--------------|---------------------|-----------------|-----------------|------------------|---------------------|-----------|-----------|------------|----------|
| LWM-Text-128k                       | 24.45 | 33.62             | 100     | 97.8          | 0.6          | 28.82               | 15.93           | 14.31           | 9.99             | 1.5                 | 0         | 3.43      | 20.05      | 1        |
| Yarn-Mistral-7b-128k                | 19.84 | 27.36             | 92.71   |               | 0            | 27.95               | 15.49           | 9.55            | 9.06             | 7.5                 | 0         | 17.14     | 0.76       | 1.25     |
| Mistral-7B-Instruct-v0.2(ABF 1000w) | 27.75 | 36.9              | 100     | 78.98         | 3.6          | 37.12               | 11.74           | 17.37           | 21.12            | 9.5                 | 0         | 29.43     | 17.51      | 0        |
| Yi-6B-200k                          | 22.15 | 32.54             | 100     | 94.92         | 0            | 36.68               | 15.07           | 9.2             | 0.92             | 3.5                 | 0         | 4.29      | 0.51       | 0.75     |
| chatglm3-6b-128k                    | 25.58 | 36.57             | 89.93   | 99.66         | 5.2          | 46.29               | 10.7            | 8.38            | 25.91            | 6.5                 | 0         | 8         | 5.33       | 1        |
| MiniCPM-2.4B-128k                   | 27.32 | 37.68             | 98.31   | 99.83         | 9            | 29.69               | 23.06           | 16.33           | 15.73            | 9.5                 | 0         | 4.29      | 22.08      | 0        |

#### MiniCPM-MoE-8x2B æ¨¡å‹è¯„æµ‹
<div align="left">

<table style="margin: 0px auto;">
<thead>
  <tr>
    <th align="left">Model</th>
    <th nowrap="nowrap" >BBH</th>
    <th nowrap="nowrap" >MMLU</th>
    <th nowrap="nowrap" >CEval</th>
    <th nowrap="nowrap" >CMMLU</th>
    <th nowrap="nowrap" >HumanEval</th>
    <th nowrap="nowrap" >MBPP&dagger;</th>
    <th nowrap="nowrap" >GSM8K</th>
    <th nowrap="nowrap" >MATH</th
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td nowrap="nowrap" align="left">Llama2-34B*</td>
    <td>44.1</td>
    <td>62.6</td>
    <td>-</td>
    <td>-</td>
    <td>22.6</td>
    <td>33.0</td>
    <td>42.2</td>
    <td>6.24</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left">Mistral-7B-Instruct-v0.2</td>
    <td>39.81</td>
    <td>60.51</td>
    <td>42.55</td>
    <td>41.92</td>
    <td>36.59</td>
    <td>39.63</td>
    <td>40.49</td>
    <td>4.95</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" >Gemma-7B*</td>
    <td>55.1</td>
    <td>64.3</td>
    <td>-</td>
    <td>-</td>
    <td>32.3</td>
    <td>44.4</td>
    <td>46.4</td>
    <td>24.3</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" >Qwen1.5-7B*</td>
    <td>40.2</td>
    <td>61</td>
    <td>74.1</td>
    <td>73.1</td>
    <td>36</td>
    <td>37.4</td>
    <td>62.5</td>
    <td>20.3</td>
  </tr>
  <tr>
    <td  nowrap="nowrap" align="left" >Deepseek-MoE(16B)*</td>
    <td>-</td>
    <td>45.0</td>
    <td>40.6</td>
    <td>42.5</td>
    <td>26.8</td>
    <td>39.2</td>
    <td>18.8</td>
    <td>4.3</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" ><b>MiniCPM-2.4B</b></td>
    <td>36.87</td>
    <td>53.46</td>
    <td>51.13</td>
    <td>51.07</td>
    <td>50.00</td>
    <td>35.93</td>
    <td>53.83</td>
    <td>10.24</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" ><b>MiniCPM-MoE-8x2B</b></td>
    <td>39.22</td>
    <td>58.90</td>
    <td>58.11</td>
    <td>58.80</td>
    <td>55.49</td>
    <td>41.68</td>
    <td>61.56</td>
    <td>10.52</td>
  </tr>
</tbody>
</table>

</div>

æ³¨ï¼š* è¡¨ç¤ºç»“æœå–è‡ªæŠ€æœ¯æŠ¥å‘Šã€‚&dagger; è¡¨ç¤ºè¯„æµ‹é›†ä¸ºMBPPå…¨é›†ã€‚

#### MiniCPM-S-1B è¯„æµ‹ç»“æœ

- ä»£ç ç”Ÿæˆï¼šåœ¨ HumanEvalï¼ˆ0-shotï¼‰å’Œ MBPPï¼ˆ3-shotï¼‰ä¸Šçš„å¹³å‡ pass@1 å¾—åˆ†ã€‚
- å¸¸è¯†æ¨ç†ï¼šåœ¨ PIQAã€SIQAã€HellaSwagã€WinoGrande å’Œ COPA ä¸Šçš„å¹³å‡ 0-shot å‡†ç¡®ç‡ã€‚
- é˜…è¯»ç†è§£ï¼šåœ¨ BoolQã€LAMBADA å’Œ TyDi QA ä¸Šçš„å¹³å‡ 0-shot å‡†ç¡®ç‡ã€‚

å…¶ä»–æµ‹è¯•é›†ï¼šæˆ‘ä»¬æŠ¥å‘Šåœ¨GSM8Kï¼ˆ8-shotï¼‰ã€MMLUï¼ˆ5-shotï¼‰ã€BBHï¼ˆ3-shotï¼‰å’Œ AGI-Evalï¼ˆ0-shotï¼‰ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ã€‚

|        Setting        | Average<br>Sparsity | Average<br>Performance | Code<br>Generation | Commonsense<br>Reasoning | Reading<br>Comprehension | GSM8K | MMLU  |  BBH  | AGI Eval |
| :-------------------: | :----------------: | :----------------------: | :----------------------: | :---: | :---: | :---: | :---------: | :-----: | :-----------------: |
| LLaMA2-7B    | - | 37.96 | 16.37 | 69.59 | 61.87 | 12.96 | 44.45 | 32.96 | 27.53 |
| ReluLLaMA-7B | 66.98 | 37.62 | 15.85 | 69.64 | 70.54 |  5.84 | 38.64 | 35.07 | 27.73 |
| **ProSparse-7B**\* | 88.11 | 38.31 | 19.47 | 66.29 | 63.33 | 12.74 | 45.21 | 33.59 | 27.55 |
| **ProSparse-7B**   | **89.32** | **38.46** | 19.42 | 66.27 | 63.50 | 12.13 | 45.48 | 34.99 | 27.46 |
| LLaMA2-13B | - | 44.06 | 20.19 | 72.58 | 71.55 | 22.21 | 54.69 | 37.89 | 29.33 |
| ReluLLaMA-13B | 71.56 | 42.74 | 20.19 | 70.44 | 73.29 | 18.50 | 50.58 | 37.97 | 28.22 |
| **ProSparse-13B**\* | 87.97 | **45.07** | 29.03 | 69.75 | 67.54 | 25.40 | 54.78 | 40.20 | 28.76 |
| **ProSparse-13B**   | **88.80** | 44.90 | 28.42 | 69.76 | 66.91 | 26.31 | 54.35 | 39.90 | 28.67 |
| MiniCPM-1B | - | 44.44 | 36.85 | 63.67 | 60.90 | 35.48 | 50.44 | 35.03 | 28.71 |
| **MiniCPM-S-1B**\*  | 86.25 | **44.72** | 41.38 | 64.55 | 60.69 | 34.72 | 49.36 | 34.04 | 28.27 |
| **MiniCPM-S-1B**    | **87.89** | **44.72** | 42.04 | 64.37 | 60.73 | 34.57 | 49.51 | 34.08 | 27.77 |

æ³¨ï¼š
1. ReluLLaMA-7B å’Œ ReluLLaMA-13B çš„ä¸‹è½½é“¾æ¥åˆ†åˆ«æ˜¯ [7B](https://huggingface.co/SparseLLM/ReluLLaMA-7B) and [13B](https://huggingface.co/SparseLLM/ReluLLaMA-13B)ã€‚"ProSparse-7B\*"ã€"ProSparse-13B\*" å’Œ "MiniCPM-S-1B\*" ä»£è¡¨æ²¡æœ‰æ¿€æ´»é˜ˆå€¼åç§»çš„ ProSparse ç‰ˆæœ¬ã€‚
2. å¯¹äº PIQAã€SIQAã€HellaSwagã€WinoGrandeã€COPAã€BoolQã€LAMBADAã€TyDi QA å’Œ AGI-Evalï¼Œæˆ‘ä»¬æ ¹æ®å„ä¸ªé€‰é¡¹çš„ PPL æ¥è¿›è¡Œç­”æ¡ˆé€‰æ‹©ã€‚å¯¹äº GSM8Kã€MMLU å’Œ BBHï¼Œæˆ‘ä»¬ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚

### æ¨¡å‹æ¨ç†

#### HuggingFaceã€vLLMæ¨ç†

å‚è€ƒ MiniCPM 1.0 ä¸­çš„[æ¨¡å‹æ¨ç†](#huggingface-æ¨ç†)éƒ¨åˆ†ã€‚

#### Powerinfer æ¨ç†

é’ˆå¯¹ MiniCPM-S-1B æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Powerinfer è¿›è¡Œæ¨ç†åŠ é€Ÿï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

1. ä¿è¯cmakeç‰ˆæœ¬3.17ä»¥ä¸Šï¼Œå¦‚æœå·²ç»å®‰è£…è¿‡ï¼Œåˆ™è·³è¿‡æ­¤æ­¥éª¤
  ```bash
    # ä¸‹è½½å®‰è£…åŒ…
    sudo wget https://cmake.org/files/v3.23/cmake-3.23.0.tar.gz
    # è§£å‹å®‰è£…åŒ…
    sudo tar -zxvf cmake-3.23.0.tar.gz
    # é…ç½®å®‰è£…ç¯å¢ƒ
    sudo ./configure
    sudo make -j8
    # ç¼–è¯‘å®‰è£…
    sudo make install
    # æŸ¥çœ‹å®‰è£…åç‰ˆæœ¬
    cmake --version
    # è¿”å›ç‰ˆæœ¬å·åˆ™å®‰è£…æˆåŠŸ
    #cmake version 3.23.0
  ```
2. å®‰è£…powerinferï¼š
```bash
  git clone https://github.com/SJTU-IPADS/PowerInfer
  cd PowerInfer
  pip install -r requirements.txt # install Python helpers' dependencies
```
3. cpuç‰ˆæœ¬powerinferç¼–è¯‘,å¦‚æœä½ çš„æœºå™¨åªæœ‰cpuï¼Œæˆ–è€…åªæƒ³ä½¿ç”¨cpuè¿›è¡Œæ¨ç†ï¼Œåˆ™è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
  cmake -S . -B build
  cmake --build build --config Release
```
4. gpuç‰ˆæœ¬powerinferç¼–è¯‘,å¦‚æœä½ çš„æœºå™¨æœ‰gpuï¼Œåˆ™å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
  cmake -S . -B build -DLLAMA_CUBLAS=ON
  cmake --build build --config Release
```
5. è·å–ç¨€ç–æ¨¡å‹
```bash
git clone https://huggingface.co/openbmb/MiniCPM-S-1B-sft-gguf/tree/main
#or
git clone https://modelscope.cn/models/OpenBMB/MiniCPM-S-1B-sft-gguf
```
6. æ¨¡å‹æ¨ç†ï¼š
```bash
cd PowerInfer
# ä»¥ä¸‹æ˜¯å‘½ä»¤æ¨¡ç‰ˆï¼Œoutput_token_countä¸ºæœ€å¤§è¾“å‡ºtokensï¼Œthread_num ä¸ºçº¿ç¨‹æ•°ï¼Œpromptä¸ºè¾“å…¥promptå­—ç¬¦
#./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt
# ä»¥ä¸‹æ˜¯ç¤ºä¾‹
./build/bin/main -m /root/ld/ld_model_pretrain/1b-s-minicpm/MiniCPM-S-1B-sft.gguf -n 2048 -t 8 -p '<ç”¨æˆ·>hello,tell me a story please.<AI>'
```
</details>

## MiniCPM 1.0

<details>
<summary>æŸ¥çœ‹ MiniCPM 1.0 çš„è¯¦ç»†ä¿¡æ¯</summary>

MiniCPM-2B è¯­è¨€æ¨¡å‹æœ‰ 24äº¿ï¼ˆ2.4Bï¼‰çš„éè¯åµŒå…¥å‚æ•°é‡, æ€»è®¡ 2.7B å‚æ•°é‡ã€‚
- ç»è¿‡ SFT åï¼ŒMiniCPM-2B åœ¨å…¬å¼€è¯„æµ‹é›†ä¸Šä¸ Mistral-7B è¡¨ç°ç›¸è¿‘ï¼ˆä¸­æ–‡ã€æ•°å­¦ã€ä»£ç èƒ½åŠ›æ›´ä¼˜ï¼‰ï¼Œæ•´ä½“æ€§èƒ½è¶…è¶Š Llama2-13Bã€MPT-30Bã€Falcon-40B ç­‰æ¨¡å‹ã€‚
- ç»è¿‡ DPO åï¼ŒMiniCPM-2B åœ¨ MTBench ä¸Šä¹Ÿè¶…è¶Šäº† Llama2-70B-Chatã€Vicuna-33Bã€Mistral-7B-Instruct-v0.1ã€Zephyr-7B-alpha ç­‰ä¼—å¤šä»£è¡¨æ€§å¼€æºå¤§æ¨¡å‹ã€‚

æ³¨æ„ï¼šä¸ºäº†ä¿è¯åœ¨å­¦æœ¯ç ”ç©¶ç”¨é€”ä¸Šæ¨¡å‹çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬**æœªå¯¹ MiniCPM-2B è¿›è¡Œä»»ä½•èº«ä»½è®¤åŒè®­ç»ƒ**ã€‚åŒæ—¶ç”±äºæˆ‘ä»¬ç”¨ ShareGPT å¼€æºè¯­æ–™ä½œä¸ºéƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¾“å‡ºç±»ä¼¼ GPT ç³»åˆ—æ¨¡å‹çš„èº«ä»½è®¤åŒä¿¡æ¯ã€‚

### è¯„æµ‹ç»“æœ

#### è¯„æµ‹è®¾ç½®

* ç”±äºå¤§æ¨¡å‹è¯„æµ‹éš¾ä»¥ç»Ÿä¸€ï¼Œä¸”å¤§é‡è¯„æµ‹ä¹Ÿæ²¡æœ‰å…¬å¼€çš„promptå’Œæµ‹è¯•ä»£ç ï¼Œå¯¹äºå…·ä½“è¯„æµ‹æ–¹å¼ï¼Œæˆ‘ä»¬åªèƒ½å°½é‡åšåˆ°é€‚åˆå„ç±»æ¨¡å‹ã€‚
* æ•´ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æµ‹è¯•æ—¶é‡‡ç”¨ç»Ÿä¸€çš„promptè¾“å…¥ï¼Œå¹¶æŒ‰ç…§å„æ¨¡å‹å¯¹åº”çš„æ¨¡æ¿è¿›è¡Œè¾“å…¥è°ƒæ•´ã€‚
* **è¯„æµ‹è„šæœ¬åŠpromptå·²å¼€æºåœ¨æˆ‘ä»¬çš„Githubä»“åº“ä¸­ï¼Œä¹Ÿæ¬¢è¿æ›´å¤šå¼€å‘è€…æ¥ä¸æ–­æ”¹è¿›æˆ‘ä»¬çš„è¯„æµ‹æ–¹å¼ã€‚**
  * æ–‡æœ¬è¯„æµ‹éƒ¨åˆ†ï¼Œé‡‡ç”¨äº†æˆ‘ä»¬çš„å¼€æºå¤§æ¨¡å‹èƒ½åŠ›è¯„æµ‹æ¡†æ¶[UltraEval](https://github.com/OpenBMB/UltraEval)ã€‚ä»¥ä¸‹ä¸ºå¼€æºæ¨¡å‹å¤ç°æµç¨‹ï¼š
    * å®‰è£…UltraEval
      ```shell
      git clone https://github.com/OpenBMB/UltraEval.git
      cd UltraEval
      pip install -e .
      ```
    * ä¸‹è½½ç›¸å…³æ•°æ®å¹¶è§£å‹å¤„ç†
      ```shell
      wget -O RawData.zip "https://cloud.tsinghua.edu.cn/f/71b5232264ae4833a4d0/?dl=1"
      unzip RawData.zip
      python data_process.py
      ```
    * æ‰§è¡Œè¯„æµ‹è„šæœ¬(æä¾›äº†æ¨¡æ¿ï¼Œå¯è‡ªå®šä¹‰)
      ```shell
      bash run_eval.sh
      ```

#### éƒ¨ç½²æ¨¡å¼

* å› ä¸ºMiniCPMé‡‡ç”¨Mupçš„ç»“æ„ï¼Œä¸ç°æœ‰æ¨¡å‹åœ¨å…·ä½“è®¡ç®—ä¸Šæœ‰ç»†å¾®å·®åˆ«ï¼Œæˆ‘ä»¬æ˜¯åŸºäºvllm=0.2.2ç‰ˆæœ¬è¿›è¡Œäº†æˆ‘ä»¬æ¨¡å‹çš„å®ç°ã€‚
* **å¯¹äºéMiniCPMæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†vllm=0.2.7çš„æœ€æ–°ç‰ˆæœ¬è¿›è¡Œæ¨ç†ã€‚**

#### è¯„æµ‹åº¦é‡

* å¯¹äºQAä»»åŠ¡ï¼ˆé€‰æ‹©é¢˜ä»»åŠ¡ï¼‰ï¼Œæˆ‘ä»¬é€‰ç”¨ä¸¤ç§æ–¹å¼è¿›è¡Œæµ‹è¯•ï¼š
  * PPLï¼šå°†é€‰é¡¹ä½œä¸ºé¢˜ç›®ç”Ÿæˆçš„å»¶ç»­ï¼Œå¹¶æ ¹æ®å„ä¸ªé€‰é¡¹çš„PPLæ¥è¿›è¡Œç­”æ¡ˆé€‰æ‹©ï¼›
  * ç¬¬äºŒç§æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆé€‰é¡¹ã€‚
* å¯¹äºä¸åŒæ¨¡å‹ï¼Œè¿™ä¸¤ç§æ–¹å¼å¾—åˆ°çš„ç»“æœå·®å¼‚è¾ƒå¤§ã€‚MiniCPMä¸¤ç§æ¨¡å¼ä¸Šçš„ç»“æœè¾ƒä¸ºæ¥è¿‘ï¼Œè€ŒMistral-7B-v0.1ç­‰æ¨¡å‹åœ¨PPLä¸Šè¡¨ç°è¾ƒå¥½ï¼Œç›´æ¥ç”Ÿæˆä¸Šæ•ˆæœè¾ƒå·®ã€‚
* åœ¨å…·ä½“è¯„æµ‹æ—¶ï¼Œæˆ‘ä»¬ä»¥ä¸¤ç§è¯„æµ‹æ–¹å¼å¾—åˆ†çš„æœ€é«˜è€…ä¸ºæœ€ç»ˆç»“æœï¼Œä»¥æ­¤ä¿è¯å¯¹æ¯”çš„å…¬å¹³æ€§(ä»¥ä¸‹è¡¨æ ¼ä¸­*å·è¡¨ç¤ºé‡‡ç”¨PPL)ã€‚

#### æ–‡æœ¬æ¨¡å‹è¯„æµ‹

**è¶Šçº§æ¯”è¾ƒ:**
|æ¨¡å‹|å¹³å‡åˆ†|è‹±æ–‡å‡åˆ†|ä¸­æ–‡å‡åˆ†|C-Eval|CMMLU|MMLU|HumanEval|MBPP|GSM8K|MATH|BBH|ARC-E|ARC-C|HellaSwag|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|Llama2-7B|35.40|36.21|31.765|32.42|31.11|44.32|12.2|27.17|13.57|1.8|33.23|75.25|42.75|75.62*|
|Qwen-7B|49.46|47.19|59.655|58.96|60.35|57.65|17.07|42.15|41.24|5.34|37.75|83.42|64.76|75.32*|
|Deepseek-7B|39.96|39.15|43.64|42.82|44.45|47.82|20.12|41.45|15.85|1.53|33.38|74.58*|42.15*|75.45*|
|Mistral-7B|48.97|49.96|44.54|46.12|42.96|62.69|27.44|45.2|33.13|5.0|41.06|83.92|70.73|80.43*|
|Llama2-13B|41.48|42.44|37.19|37.32|37.06|54.71|17.07|32.55|21.15|2.25|37.92|78.87*|58.19|79.23*|
|MPT-30B|38.17|39.82|30.72|29.34|32.09|46.56|21.95|35.36|10.31|1.56|38.22|78.66*|46.08*|79.72*|
|Falcon-40B|43.62|44.21|40.93|40.29|41.57|53.53|24.39|36.53|22.44|1.92|36.24|81.94*|57.68|83.26*|
|MiniCPM-2B|52.33|52.6|51.1|51.13|51.07|53.46|50.00|47.31|53.83|10.24|36.87|85.44|68.00|68.25|

**åŒçº§æ¯”è¾ƒï¼š**
|æ¨¡å‹|å¹³å‡åˆ†|è‹±æ–‡å‡åˆ†|ä¸­æ–‡å‡åˆ†|C-Eval|CMMLU|MMLU|HumanEval|MBPP|GSM8K|MATH|BBH|ARC-E|ARC-C|HellaSwag|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|TinyLlama-1.1B|25.36|25.55|24.525|25.02|24.03|24.3|6.71|19.91|2.27|0.74|28.78|60.77*|28.15*|58.33*|Qwen-1.8B|34.72|31.87|47.565|49.81|45.32|43.37|7.93|17.8|19.26|2.42|29.07|63.97*|43.69|59.28*|
|Qwen-1.8B|34.72|31.87|47.57|49.81|45.32|43.37|7.93|17.80|19.26|2.42|29.07|63.97*|43.69|59.28*|
|Gemini Nano-3B|-|-|-|-|-|-|-|27.2(report)|22.8(report)|-|42.4(report)|-|-|-|
|StableLM-Zephyr-3B|43.46|46.31|30.62|30.34|30.89|45.9|35.37|31.85|52.54|12.49|37.68|73.78|55.38|71.87*|
|Phi-2-2B|48.84|54.41|23.78|23.37|24.18|52.66|47.56|55.04|57.16|3.5|43.39|86.11|71.25|73.07*|
|MiniCPM-2B|52.33|52.6|51.10|51.13|51.07|53.46|50.00|47.31|53.83|10.24|36.87|85.44|68.00|68.25|

**Chatæ¨¡å‹æ¯”è¾ƒï¼š**
|æ¨¡å‹|å¹³å‡åˆ†|è‹±æ–‡å‡åˆ†|ä¸­æ–‡å‡åˆ†|C-Eval|CMMLU|MMLU|HumanEval|MBPP|GSM8K|MATH|BBH|ARC-E|ARC-C|HellaSwag|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|ChatGLM2-6B|37.98|35.17|50.63|52.05|49.21|45.77|10.37|9.38|22.74|5.96|32.6|74.45|56.82|58.48*|
|Mistral-7B-Instruct-v0.1|44.36|45.89|37.51|38.06|36.96|53.56|29.27|39.34|28.73|3.48|39.52|81.61|63.99|73.47*|
|Mistral-7B-Instruct-v0.2|50.91|52.83|42.235|42.55|41.92|60.51|36.59|48.95|40.49|4.95|39.81|86.28|73.38|84.55*|
|Qwen-7B-Chat|44.93|42.05|57.9|58.57|57.23|56.03|15.85|40.52|42.23|8.3|37.34|64.44*|39.25*|74.52*|
|Yi-6B-Chat|50.46|45.89|70.995|70.88|71.11|62.95|14.02|28.34|36.54|3.88|37.43|84.89|70.39|74.6*|
|Baichuan2-7B-Chat|44.68|42.74|53.39|53.28|53.5|53|21.34|32.32|25.25|6.32|37.46|79.63|60.15|69.23*|
|Deepseek-7B-chat|49.34|49.56|48.335|46.95|49.72|51.67|40.85|48.48|48.52|4.26|35.7|76.85|63.05|76.68*|
|Llama2-7B-Chat|38.16|39.17|33.59|34.54|32.64|47.64|14.02|27.4|21.15|2.08|35.54|74.28|54.78|75.65*|
|MiniCPM-2B|52.33|52.6|51.10|51.13|51.07|53.46|50.00|47.31|53.83|10.24|36.87|85.44|68.00|68.25|

**DPOåæ¨¡å‹æ¯”è¾ƒï¼š**

|æ¨¡å‹|MT-bench|
|---|---|
|GPT-4-turbo|9.32|
|GPT-3.5-turbo|8.39|
|Mistral-8*7b-Instruct-v0.1|8.30|
|Claude-2.1|8.18|
|Zephyr-7B-beta|7.34|
|**MiniCPM-2B**|**7.25**|
|Vicuna-33B|7.12|
|Zephyr-7B-alpha|6.88|
|LLaMA-2-70B-chat|6.86|
|Mistral-7B-Instruct-v0.1|6.84|
|MPT-34B-instruct|6.39|


### å¿«é€Ÿä¸Šæ‰‹ 

#### åœ¨çº¿ä½“éªŒ

- [Colab](https://colab.research.google.com/drive/1tJcfPyWGWA5HezO7GKLeyeIso0HyOc0l?usp=sharing)

#### åŸºäºGradioçš„ç½‘é¡µç‰ˆDemo

* ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¯åŠ¨åŸºäºGradioçš„ç½‘é¡µç‰ˆdemoï¼š

```shell
# generation powered by vllm
python demo/minicpm/vllm_based_demo.py --model_path <vllmcpm_repo_path>
# generation powered by huggingface
python demo/minicpm/hf_based_demo.py --model_path <hf_repo_path>
```

#### HuggingFace æ¨ç†

##### MiniCPM-2B

å®‰è£…`transformers>=4.36.0`ä»¥åŠ`accelerate`åï¼Œè¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(0)

path = 'openbmb/MiniCPM-2B-dpo-bf16'
tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)

responds, history = model.chat(tokenizer, "å±±ä¸œçœæœ€é«˜çš„å±±æ˜¯å“ªåº§å±±, å®ƒæ¯”é»„å±±é«˜è¿˜æ˜¯çŸ®ï¼Ÿå·®è·å¤šå°‘ï¼Ÿ", temperature=0.5, top_p=0.8, repetition_penalty=1.02)
print(responds)
```

##### MiniCPM-2B ï¼ˆLlama Formatï¼‰

æˆ‘ä»¬å°†MiniCPMçš„æ¨¡å‹æƒé‡è½¬åŒ–æˆäº†Llamaä»£ç å¯ä»¥ç›´æ¥è°ƒç”¨çš„[æ ¼å¼](https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16-llama-format)ï¼Œä»¥ä¾¿å¤§å®¶å°è¯•:

```python
import torch
from transformers import LlamaTokenizerFast, LlamaForCausalLM
model_path = "openbmb/MiniCPM-2B-dpo-bf16-llama-format"
tokenizer = LlamaTokenizerFast.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)

prompt="Now you act like a terminal situated within a beginner's C++ practice repository folder, please provide the output for the command: `ls -l`"
input_ids = tokenizer.encode("<ç”¨æˆ·>{}<AI>".format(prompt), return_tensors='pt', add_special_tokens=True).cuda()
responds = model.generate(input_ids, temperature=0.3, top_p=0.8, repetition_penalty=1.02, max_length=1024)
responds = tokenizer.decode(responds[0], skip_special_tokens=True)
print(responds)
```

#### vLLM æ¨ç†

å®‰è£… [vLLM](https://github.com/vllm-project/vllm)ã€‚

```shell
pip install "vllm>=0.4.1"
```

å…·ä½“æ¨ç†ä»£ç è§[è¿™é‡Œ](#vllm)ã€‚

#### SGLang æ¨ç†

å®‰è£… [SGLang](https://github.com/sgl-project/sglang)ã€‚

* é¦–å…ˆéœ€è¦å¯åŠ¨ä¸€ä¸ªæœåŠ¡:

```bash
python -m sglang.launch_server --model-path openbmb/MiniCPM-2B-dpo-fp16 --trust-remote-code --port 30000
```

* ä¸‹é¢æ˜¯ä¸€ä¸ªæ¨ç†ä»£ç çš„æ ·ä¾‹:

```python
from sglang import function, gen, set_default_backend, RuntimeEndpoint

@function
def text_qa(s, question):
    s += "<ç”¨æˆ·>" + question + "<AI>"
    s += gen("answer", max_tokens=1024, temperature=0.7, top_p=0.7)

set_default_backend(RuntimeEndpoint("http://localhost:30000"))

state = text_qa.run(
    question="What is the capital of China?",
)

print(state["answer"])
```

#### llama.cppã€Ollamaã€fastllmã€mlx_lmæ¨ç†
MiniCPMæ”¯æŒ[llama.cpp](https://github.com/ggerganov/llama.cpp/) ã€[ollama](https://github.com/ollama/ollama)ã€[fastllm](https://github.com/ztxz16/fastllm)ã€[mlx_lm](https://github.com/ml-explore/mlx-examples)æ¨ç†ã€‚æ„Ÿè°¢[@runfuture](https://github.com/runfuture)å¯¹llama.cppå’Œollamaçš„é€‚é…ã€‚

è¯·å‚è€ƒ MiniCPM çŸ¥è¯†åº“ä¸­çš„[è¾¹ç«¯éƒ¨ç½²æ•™ç¨‹](https://modelbest.feishu.cn/wiki/VL5kw9DsEiRDmJkEyTUcydE0nie)ã€‚

#### æ¨¡å‹é‡åŒ–

è¯·å‚è€ƒ MiniCPM çŸ¥è¯†åº“ä¸­çš„[é‡åŒ–æŒ‡å—](https://modelbest.feishu.cn/wiki/EatbwdLuvitbbMk2X5wcX6h5n7c)ã€‚

#### æ¨¡å‹å¾®è°ƒ

- ä¸€å¼  1080/2080 å¯å®ç°é«˜æ•ˆå‚æ•°å¾®è°ƒï¼š[ä»£ç ](https://github.com/OpenBMB/MiniCPM/tree/main/finetune)
- mlx å¾®è°ƒï¼š[æ•™ç¨‹](https://modelbest.feishu.cn/wiki/AIU3wbREcirOm9kkvd7cxujFnMb#share-ASrDdvFAloHtycxfy85cLNhAnd3)
- [xtuner](https://github.com/InternLM/xtuner): [MiniCPMé«˜æ•ˆç‡å¾®è°ƒçš„ä¸äºŒé€‰æ‹©](https://modelbest.feishu.cn/wiki/AIU3wbREcirOm9kkvd7cxujFnMb#AMdXdzz8qoadZhxU4EucELWznzd)
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory.git)ï¼š[MiniCPMå¾®è°ƒä¸€é”®å¼è§£å†³æ–¹æ¡ˆ](https://modelbest.feishu.cn/wiki/AIU3wbREcirOm9kkvd7cxujFnMb#BAWrdSjXuoFvX4xuIuzc8Amln5E)

</details>


## å¼€æºåè®®

#### æ¨¡å‹åè®®

* æœ¬ä»“åº“ä¸­ä»£ç ä¾ç…§ [Apache-2.0](https://github.com/OpenBMB/MiniCPM/blob/main/LICENSE) åè®®å¼€æº
* MiniCPM æ¨¡å‹æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [MiniCPM æ¨¡å‹å•†ç”¨è®¸å¯åè®®](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%E6%A8%A1%E5%9E%8B%E5%95%86%E7%94%A8%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.md)ã€‚
* MiniCPM æ¨¡å‹æƒé‡å¯¹å­¦æœ¯ç ”ç©¶å®Œå…¨å¼€æ”¾ï¼Œåœ¨å¡«å†™[é—®å·](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g)è¿›è¡Œç™»è®°åäº¦å…è®¸å…è´¹å•†ä¸šä½¿ç”¨ã€‚

#### å£°æ˜

* ä½œä¸ºä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ŒMiniCPM é€šè¿‡å­¦ä¹ å¤§é‡çš„æ–‡æœ¬æ¥ç”Ÿæˆå†…å®¹ï¼Œä½†å®ƒæ— æ³•ç†è§£ã€è¡¨è¾¾ä¸ªäººè§‚ç‚¹æˆ–ä»·å€¼åˆ¤æ–­ï¼Œå®ƒæ‰€è¾“å‡ºçš„ä»»ä½•å†…å®¹éƒ½ä¸ä»£è¡¨æ¨¡å‹å¼€å‘è€…çš„è§‚ç‚¹å’Œç«‹åœºã€‚
* å› æ­¤ç”¨æˆ·åœ¨ä½¿ç”¨ MiniCPM ç”Ÿæˆçš„å†…å®¹æ—¶ï¼Œåº”è‡ªè¡Œè´Ÿè´£å¯¹å…¶è¿›è¡Œè¯„ä¼°å’ŒéªŒè¯ã€‚
* å¦‚æœç”±äºä½¿ç”¨ MiniCPM å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

## å¼€å‘æœºæ„

æœ¬é¡¹ç›®ç”±ä»¥ä¸‹æœºæ„å…±åŒå¼€å‘ï¼š

- <img src="assets/modelbest.png" width="28px"> [é¢å£æ™ºèƒ½](https://modelbest.cn/)
- <img src="assets/thunlp.png" width="28px"> [æ¸…åå¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤](https://nlp.csai.tsinghua.edu.cn/)

## å·¥ä½œå¼•ç”¨

* å¦‚æœè§‰å¾—MiniCPMæœ‰åŠ©äºæ‚¨çš„å·¥ä½œï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2404.06395)

```
@article{hu2024minicpm,
  title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}
```

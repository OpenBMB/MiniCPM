<div align="center">
<img src="./assets/minicpm_logo.png" width="500em" ></img> 
</div>

<h4 align="center">
    <p>
        <b>ä¸­æ–‡</b> | <a href="https://github.com/OpenBMB/MiniCPM/blob/main/README-en.md">English</a>
    <p>
</h4>


<p align="center">
<a href="https://openbmb.vercel.app/?category=Chinese+Blog" target="_blank">MiniCPM æŠ€æœ¯åšå®¢</a> |
<a href="https://modelbest.feishu.cn/wiki/D2tFw8Pcsi5CIzkaHNacLK64npg" target="_blank">MiniCPM çŸ¥è¯†åº“</a> |
<a href="https://arxiv.org/abs/2404.06395" target="_blank">MiniCPM è®ºæ–‡</a> |
<a href="https://github.com/OpenBMB/MiniCPM-V/" target="_blank">MiniCPM-V ä»“åº“</a> |
åŠ å…¥æˆ‘ä»¬çš„ <a href="https://discord.gg/3cGQn9b3YM" target="_blank">discord</a> å’Œ <a href="https://github.com/OpenBMB/MiniCPM/blob/main/assets/wechat.jpg" target="_blank">å¾®ä¿¡ç¾¤</a>
 
</p>

## æ›´æ–°æ—¥å¿—ğŸ”¥
- [2025.06.06] å‘å¸ƒ [**MiniCPM4**](https://huggingface.co/collections/openbmb/minicpm-4-6841ab29d180257e940baa9b)ï¼è¯¥æ¨¡å‹åœ¨ä¿æŒåŒç­‰è§„æ¨¡æœ€ä¼˜æ€§èƒ½çš„åŒæ—¶ï¼Œå®ç°äº†æè‡´çš„æ•ˆç‡æå‡ï¼åœ¨å…¸å‹ç«¯ä¾§èŠ¯ç‰‡ä¸Šèƒ½å¤Ÿå®ç° 5 å€ä»¥ä¸Šç”ŸæˆåŠ é€Ÿï¼
- [2024.09.28] **[LLMxMapReduce](https://github.com/thunlp/LLMxMapReduce)** å¼€æºï¼Œæ”¯æŒ MiniCPM3-4Bï¼Œç†è®ºä¸Šæ”¯æŒæ— é™é•¿æ–‡æœ¬è¾“å…¥ï¼
- [2024.09.18] **[SGLang](https://github.com/sgl-project/sglang) å·²ç»æ”¯æŒ MiniCPM3-4B (æ¨èä½¿ç”¨)ï¼ç”±äº SGLang v0.3 å¯¹ MiniCPM3 ä¸­ä½¿ç”¨çš„ MLA ç»“æ„è¿›è¡Œäº†æ¨ç†ä¼˜åŒ–ï¼Œååé‡ç›¸æ¯”äº vLLM æé«˜ 70%ï¼**[[ç”¨æ³•](#sglangæ¨è)]
- [2024.09.16] [llama.cpp](https://github.com/ggerganov/llama.cpp/releases/tag/b3765) å·²ç»å®˜æ–¹æ”¯æŒ MiniCPM3-4Bï¼[[GGUFæ¨¡å‹](https://huggingface.co/openbmb/MiniCPM3-4B-GGUF)|[ç”¨æ³•](#llamacpp)]
- [2024.09.05] å‘å¸ƒ [**MiniCPM3-4B**](https://huggingface.co/openbmb/MiniCPM3-4B)ï¼è¯¥æ¨¡å‹çš„è¡¨ç°è¶…è¶Š Phi-3.5-mini-instruct å’Œ GPT-3.5-Turbo-0125ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¯”è‚© Llama3.1-8B-Instructã€Qwen2-7B-Instructã€GLM-4-9B-Chat ç­‰å¤šä¸ª 7B-9B å‚æ•°é‡çš„æ¨¡å‹ã€‚
- [2024.07.09] MiniCPM-2B å·²ç»æ”¯æŒä½¿ç”¨ [SGLang](#sglang-æ¨ç†) æ¨ç†ï¼
- [2024.07.05] å‘å¸ƒ [MiniCPM-S-1B](https://huggingface.co/openbmb/MiniCPM-S-1B-sft)ï¼è¯¥æ¨¡å‹åœ¨ä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ— æŸçš„å‰æä¸‹ï¼ŒFFN å±‚å®ç°äº† 87.89% çš„å¹³å‡ç¨€ç–åº¦ï¼Œå°† FFN FLOPs é™ä½äº† 84%ã€‚
- [2024.04.11] å‘å¸ƒ [MiniCPM-2B-128k](https://huggingface.co/openbmb/MiniCPM-2B-128k)ã€[MiniCPM-MoE-8x2B](https://huggingface.co/openbmb/MiniCPM-MoE-8x2B) å’Œ [MiniCPM-1B](https://huggingface.co/openbmb/MiniCPM-1B-sft-bf16)ï¼ç‚¹å‡»[è¿™é‡Œ](https://openbmb.vercel.app/?category=Chinese+Blog)æŸ¥çœ‹æŠ€æœ¯åšå®¢ã€‚
- [2024.03.16] MiniCPM-2B çš„ 30 ä½™ä¸ªä¸­é—´æ£€æŸ¥ç‚¹å¼€æ”¾äº†ï¼[HuggingFaceé“¾æ¥](https://huggingface.co/openbmb/MiniCPM-2B-history)
- [2024.02.01] å‘å¸ƒ [**MiniCPM-2B**](https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16)ï¼è¯¥æ¨¡å‹åœ¨å…¬å¼€è¯„æµ‹é›†ä¸Šä¸ Mistral-7B è¡¨ç°ç›¸è¿‘ï¼ˆä¸­æ–‡ã€æ•°å­¦ã€ä»£ç èƒ½åŠ›æ›´ä¼˜ï¼‰ï¼Œæ•´ä½“æ€§èƒ½è¶…è¶Š Llama2-13Bã€MPT-30Bã€Falcon-40B ç­‰æ¨¡å‹ã€‚

## ç›®å½•

- [æ›´æ–°æ—¥å¿—ğŸ”¥](#æ›´æ–°æ—¥å¿—)
- [ç›®å½•](#ç›®å½•)
- [æ¨¡å‹ä¸‹è½½](#æ¨¡å‹ä¸‹è½½)
- [MiniCPM 4.0](#minicpm-40)
  - [è¯„æµ‹ç»“æœ](#è¯„æµ‹ç»“æœ)
    - [æ•ˆç‡è¯„æµ‹](#æ•ˆç‡è¯„æµ‹)
    - [ç»¼åˆè¯„æµ‹](#ç»¼åˆè¯„æµ‹)
    - [é•¿æ–‡æœ¬è¯„æµ‹](#é•¿æ–‡æœ¬è¯„æµ‹)
  - [BitCPM4: æ¨¡å‹é‡åŒ–](#bitcpm4-æ¨¡å‹é‡åŒ–)
    - [BitCPM4è¯„æµ‹](#bitcpm4è¯„æµ‹)
    - [BitCPM4æ¨¡å‹æ¨ç†](#bitcpm4æ¨¡å‹æ¨ç†)
  - [æ¨¡å‹åº”ç”¨](#æ¨¡å‹åº”ç”¨)
    - [MiniCPM4-Survey: ç»¼è¿°ç”Ÿæˆ](#minicpm4-survey-ç»¼è¿°ç”Ÿæˆ)
    - [MiniCPM4-MCP: MCPå¢å¼ºçš„å·¥å…·è°ƒç”¨](#minicpm4-mcp-mcpå¢å¼ºçš„å·¥å…·è°ƒç”¨)
  - [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
    - [CPM.cu](#cpmcu)
    - [HuggingFace](#huggingface)
    - [vLLM](#vllm)
    - [SGLang](#sglang)
    - [llama.cpp](#llamacpp)
  - [æ¨¡å‹å¾®è°ƒ](#æ¨¡å‹å¾®è°ƒ)
    - [LLaMA-Factory](#llama-factory)
    - [XTuner](#xtuner)
- [MiniCPM 3.0](#minicpm-30)
- [MiniCPM 2.0](#minicpm-20)
- [MiniCPM 1.0](#minicpm-10)
- [å¼€æºåè®®](#å¼€æºåè®®)
- [å¼€å‘æœºæ„](#å¼€å‘æœºæ„)
- [å·¥ä½œå¼•ç”¨](#å·¥ä½œå¼•ç”¨)


## æ¨¡å‹ä¸‹è½½
 
  | HuggingFace | ModelScope |
  |-------------|------------|
  | [MiniCPM4-8B](https://huggingface.co/openbmb/MiniCPM4-8B)    | [MiniCPM4-8B](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-8B) |
  | [MiniCPM4-0.5B](https://huggingface.co/openbmb/MiniCPM4-0.5B) | [MiniCPM4-0.5B](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-0.5B) |
  | [BitCPM4-1B](https://huggingface.co/openbmb/BitCPM4-1B)        | [BitCPM4-1B](https://www.modelscope.cn/models/OpenBMB/BitCPM4-1B) |
  | [BitCPM4-0.5B](https://huggingface.co/openbmb/BitCPM4-0.5B)    | [BitCPM4-0.5B](https://www.modelscope.cn/models/OpenBMB/BitCPM4-0.5B) |
  | [MiniCPM4-8B-Eagle-FRSpec](https://huggingface.co/openbmb/MiniCPM4-8B-Eagle-FRSpec) | [MiniCPM4-8B-Eagle-FRSpec](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-8B-Eagle-FRSpec) |
  | [MiniCPM4-8B-Eagle-FRSpec-QAT](https://huggingface.co/openbmb/MiniCPM4-8B-Eagle-FRSpec-QAT) | [MiniCPM4-8B-Eagle-FRSpec-QAT](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-8B-Eagle-FRSpec-QAT) |
  | [MiniCPM4-8B-Eagle-vLLM](https://huggingface.co/openbmb/MiniCPM4-8B-Eagle-vLLM) | [MiniCPM4-8B-Eagle-vLLM](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-8B-Eagle-vLLM) |
  | [MiniCPM4-8B-marlin-Eagle-vLLM](https://huggingface.co/openbmb/MiniCPM4-8B-marlin-Eagle-vLLM) | [MiniCPM4-8B-marlin-Eagle-vLLM](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-8B-marlin-Eagle-vLLM) |
  | [MiniCPM4-Survey](https://huggingface.co/openbmb/MiniCPM4-Survey) | [MiniCPM4-Survey](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-Survey) |
  | [MiniCPM4-MCP](https://huggingface.co/openbmb/MiniCPM4-MCP)  | [MiniCPM4-MCP](https://www.modelscope.cn/models/OpenBMB/MiniCPM4-MCP) |
  |[MiniCPM3-4B](https://huggingface.co/openbmb/MiniCPM3-4B)|[MiniCPM3-4B](https://www.modelscope.cn/models/OpenBMB/MiniCPM3-4B)|
  |[MiniCPM-2B-sft](https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16)|[MiniCPM-2B-sft](https://modelscope.cn/models/OpenBMB/miniCPM-bf16)|
  |[MiniCPM-2B-dpo](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16)|[MiniCPM-2B-dpo](https://modelscope.cn/models/OpenBMB/MiniCPM-2B-dpo-bf16/summary)|
  |[MiniCPM-2B-128k](https://huggingface.co/openbmb/MiniCPM-2B-128k) |[MiniCPM-2B-128k](https://modelscope.cn/models/openbmb/MiniCPM-2B-128k/summary)| 
  |[MiniCPM-MoE-8x2B](https://huggingface.co/openbmb/MiniCPM-MoE-8x2B) |[MiniCPM-MoE-8x2B](https://modelscope.cn/models/OpenBMB/MiniCPM-MoE-8x2B)| 
  |[MiniCPM-1B](https://huggingface.co/openbmb/MiniCPM-1B-sft-bf16) | [MiniCPM-1B](https://modelscope.cn/models/OpenBMB/MiniCPM-1B-sft-bf16) |
  |[MiniCPM-S-1B](https://huggingface.co/openbmb/MiniCPM-S-1B-sft)|[MiniCPM-S-1B](https://modelscope.cn/models/OpenBMB/MiniCPM-S-1B-sft)|

  æ³¨: æ›´å¤šæ¨¡å‹ç‰ˆæœ¬è§[è¿™é‡Œ](https://huggingface.co/collections/openbmb/minicpm-2b-65d48bf958302b9fd25b698f)ã€‚

## MiniCPM 4.0
MiniCPM 4 æ˜¯ä¸€ä¸ªæè‡´é«˜æ•ˆçš„ç«¯ä¾§å¤§æ¨¡å‹ï¼Œä»æ¨¡å‹æ¶æ„ã€å­¦ä¹ ç®—æ³•ã€è®­ç»ƒæ•°æ®ä¸æ¨ç†ç³»ç»Ÿå››ä¸ªå±‚é¢è¿›è¡Œäº†é«˜æ•ˆä¼˜åŒ–ï¼Œå®ç°äº†æè‡´çš„æ•ˆç‡æå‡ã€‚
- ğŸ—ï¸ é«˜æ•ˆæ¨¡å‹æ¶æ„ï¼š
  - InfLLM v2 -- å¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼šé‡‡ç”¨å¯è®­ç»ƒçš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶æ¶æ„ï¼Œåœ¨ 128K é•¿æ–‡æœ¬å¤„ç†ä¸­ï¼Œæ¯ä¸ªè¯å…ƒä»…éœ€ä¸ä¸è¶³ 5% çš„è¯å…ƒè¿›è¡Œç›¸å…³æ€§è®¡ç®—ï¼Œæ˜¾è‘—é™ä½é•¿æ–‡æœ¬çš„è®¡ç®—å¼€é”€
- ğŸ§  é«˜æ•ˆå­¦ä¹ ç®—æ³•ï¼š
  - æ¨¡å‹é£æ´ 2.0 -- é«˜æ•ˆ Predictable Scalingï¼šå¼•å…¥ä¸‹æ¸¸ä»»åŠ¡çš„ Scaling é¢„æµ‹æ–¹æ³•ï¼Œå®ç°æ›´ç²¾å‡†çš„æ¨¡å‹è®­ç»ƒé…ç½®æœç´¢
  - BitCPM -- æè‡´çš„ä¸‰å€¼é‡åŒ–ï¼šå°†æ¨¡å‹å‚æ•°ä½å®½å‹ç¼©è‡³ 3 å€¼ï¼Œå®ç°æ¨¡å‹ä½å®½ 90% çš„æè‡´ç˜¦èº«
  - é«˜æ•ˆè®­ç»ƒå·¥ç¨‹ä¼˜åŒ–ï¼šé‡‡ç”¨ FP8 ä½ç²¾åº¦è®¡ç®—æŠ€æœ¯ï¼Œç»“åˆå¤šè¯å…ƒé¢„æµ‹ï¼ˆMulti-token Predictionï¼‰è®­ç»ƒç­–ç•¥
- ğŸ“š é«˜çŸ¥è¯†å¯†åº¦è®­ç»ƒæ•°æ®ï¼š
  - UltraClean -- é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®çš„æ¸…æ´—ä¸åˆæˆï¼šæ„å»ºåŸºäºé«˜æ•ˆéªŒè¯çš„è¿­ä»£å¼æ•°æ®æ¸…æ´—ç­–ç•¥ï¼Œå¼€æºé«˜è´¨é‡ä¸­è‹±æ–‡é¢„è®­ç»ƒæ•°æ®é›† [UltraFineweb](https://huggingface.co/datasets/openbmb/Ultra-FineWeb)
  - UltraChat v2 -- é«˜è´¨é‡æœ‰ç›‘ç£å¾®è°ƒæ•°æ®åˆæˆï¼šæ„å»ºå¤§è§„æ¨¡é«˜è´¨é‡æœ‰ç›‘ç£å¾®è°ƒæ•°æ®é›†ï¼Œæ¶µç›–çŸ¥è¯†å¯†é›†å‹æ•°æ®ã€æ¨ç†å¯†é›†å‹æ•°æ®ã€æŒ‡ä»¤éµå¾ªæ•°æ®ã€é•¿æ–‡æœ¬ç†è§£æ•°æ®ã€å·¥å…·è°ƒç”¨æ•°æ®ç­‰å¤šä¸ªç»´åº¦
- âš¡ é«˜æ•ˆæ¨ç†ç³»ç»Ÿï¼š
  - FRSpec -- è½»é‡çº§æŠ•æœºé‡‡æ ·ï¼šé€šè¿‡å¯¹è‰ç¨¿æ¨¡å‹è¯è¡¨çš„æ™ºèƒ½è£å‰ªï¼Œå®ç°è‰ç¨¿æ¨¡å‹çš„ç”ŸæˆåŠ é€Ÿä¼˜åŒ–
  - ArkInfer -- è·¨å¹³å°éƒ¨ç½²ç³»ç»Ÿï¼šæ”¯æŒå¤šåç«¯ç¯å¢ƒçš„ä¸€é”®éƒ¨ç½²ï¼Œæä¾›çµæ´»çš„è·¨å¹³å°é€‚é…èƒ½åŠ›

### è¯„æµ‹ç»“æœ
#### æ•ˆç‡è¯„æµ‹
åœ¨ Jetson AGX Orin å’Œ RTX 4090 ä¸¤æ¬¾å…¸å‹ç«¯ä¾§èŠ¯ç‰‡ä¸Šï¼ŒMiniCPM4 åœ¨é•¿æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­å±•ç°å‡ºå¤§å¹…é¢†å…ˆåŒå°ºå¯¸æ¨¡å‹çš„å¤„ç†é€Ÿåº¦ã€‚éšç€æ–‡æœ¬é•¿åº¦çš„å¢åŠ ï¼ŒMiniCPM4 çš„æ€§èƒ½ä¼˜åŠ¿æ„ˆå‘æ˜¾è‘—ã€‚åœ¨ Jetson AGX Orin å¹³å°ä¸Šï¼Œç›¸è¾ƒäº Qwen3-8Bï¼ŒMiniCPM4 å®ç°äº†çº¦ 7 å€çš„ç”Ÿæˆé€Ÿåº¦æå‡ã€‚

![benchmark](./assets/minicpm4/efficiency.png)

#### ç»¼åˆè¯„æµ‹
MiniCPM4 æ¨å‡ºç«¯ä¾§ 8Bã€0.5B ä¸¤ç§å‚æ•°è§„æ¨¡ç‰ˆæœ¬ï¼Œå‡åœ¨åŒçº§åˆ«æ¨¡å‹ä¸­å®ç°äº†æœ€ä½³æ€§èƒ½è¡¨ç°ã€‚
![benchmark](./assets/minicpm4/benchmark.png)

#### é•¿æ–‡æœ¬è¯„æµ‹
MiniCPM4 åŸºäº 32K é•¿æ–‡æœ¬è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡ YaRN æŠ€æœ¯å®ç°é•¿åº¦æ‰©å±•ã€‚åœ¨ 128K é•¿æ–‡æœ¬çš„å¤§æµ·æé’ˆä»»åŠ¡ä¸­ï¼ŒMiniCPM4 å±•ç°å‡ºå“è¶Šçš„æ€§èƒ½è¡¨ç°ã€‚

![long-niah](./assets/minicpm4/128k-niah.png)

### BitCPM4: æ¨¡å‹é‡åŒ–
BitCPM4 æ˜¯åŸºäº MiniCPM ç³»åˆ—æ¨¡å‹è¿›è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰åå¾—åˆ°çš„ä¸‰å€¼é‡åŒ–æ¨¡å‹ï¼Œåœ¨è®­ç»ƒæ•ˆç‡å’Œæ¨¡å‹å‚æ•°æ•ˆç‡å®ç°äº†æœ‰æ•ˆçš„æå‡ã€‚
- è®­ç»ƒæ–¹æ³•æ”¹è¿›
  - åœ¨å°è§„æ¨¡æ¨¡å‹ä¸Šè¿›è¡Œé£æ´å®éªŒï¼Œæœç´¢è®­ç»ƒæ‰€éœ€çš„è®­ç»ƒè¶…å‚ã€‚
  - é€šè¿‡ä½¿ç”¨ä¸€é˜¶æ®µé«˜ç²¾è®­ç»ƒ+äºŒé˜¶æ®µ QAT çš„æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨å·²ç»å®Œæˆæˆ–éƒ¨åˆ†å®Œæˆè®­ç»ƒçš„é«˜ç²¾åº¦æ¨¡å‹ï¼Œæå¤§åœ°å‹ç¼©äº† QAT é˜¶æ®µæ‰€éœ€è¦çš„ç®—åŠ›ã€‚
- é«˜æ•ˆå‚æ•°æ•ˆç‡
  - æ¨¡å‹ä½¿ç”¨ 1.58Bit çš„ä½å®½è¾¾åˆ°çš„æ€§èƒ½å¯¹æ ‡ä¸åŒå‚æ•°é‡çº§åˆ«çš„å…¨ç²¾åº¦æ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°æ•ˆç‡é«˜ã€‚

#### BitCPM4 è¯„æµ‹
BitCPM4 åœ¨æµ‹è¯•ä¸­çš„è¡¨ç°å¯ä»¥å¯¹æ ‡åŒçº§åˆ«çš„ä¸šç•Œä¸»æµå…¨ç²¾åº¦æ¨¡å‹ã€‚
![bitcpm-benchmark](./assets/minicpm4/bitcpm4-benchmark.png)

#### BitCPM4 æ¨¡å‹æ¨ç†
BitCPM4 å¼€æºçš„æ¨¡å‹å‚æ•°ä¸ºä¼ªé‡åŒ–å½¢å¼ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ Huggingface æ¡†æ¶è¿›è¡Œæ¨ç†ã€‚

### æ¨¡å‹åº”ç”¨

#### MiniCPM4-Survey: ç»¼è¿°ç”Ÿæˆ
MiniCPM4-Survey æ˜¯ç”± [THUNLP](https://nlp.csai.tsinghua.edu.cn)ã€ä¸­å›½äººæ°‘å¤§å­¦å’Œ [ModelBest](https://modelbest.cn/en) è”åˆå¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ã€‚å®ƒåŸºäº MiniCPM4-8B åŸºåº§æ¨¡å‹ï¼Œæ¥å—ç”¨æˆ·è´¨é‡ä½œä¸ºè¾“å…¥ï¼Œè‡ªä¸»ç”Ÿæˆå¯ä¿¡çš„é•¿ç¯‡ç»¼è¿°è®ºæ–‡ã€‚
ä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š
- è®¡åˆ’-æ£€ç´¢-å†™ä½œç”Ÿæˆæ¡†æ¶ â€” æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šæ™ºèƒ½ä½“ç”Ÿæˆæ¡†æ¶ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µï¼šè®¡åˆ’ï¼ˆå®šä¹‰ç»¼è¿°çš„æ•´ä½“ç»“æ„ï¼‰ã€æ£€ç´¢ï¼ˆç”Ÿæˆåˆé€‚çš„æ£€ç´¢å…³é”®è¯ï¼‰å’Œå†™ä½œï¼ˆåˆ©ç”¨æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œç”Ÿæˆè¿è´¯çš„æ®µè½ï¼‰ã€‚
- é«˜è´¨é‡æ•°æ®é›†æ„å»ºâ€”â€”æˆ‘ä»¬æ”¶é›†å¹¶å¤„ç†å¤§é‡äººç±»ä¸“å®¶å†™ä½œçš„ç»¼è¿°è®ºæ–‡ï¼Œæ„å»ºé«˜è´¨é‡è®­ç»ƒé›†ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æ”¶é›†å¤§é‡ç ”ç©¶è®ºæ–‡ï¼Œæ„å»ºæ£€ç´¢æ•°æ®åº“ã€‚
- å¤šæ–¹é¢å¥–åŠ±è®¾è®¡ â€” æˆ‘ä»¬ç²¾å¿ƒè®¾è®¡äº†åŒ…å«ç»“æ„ã€å†…å®¹å’Œå¼•ç”¨çš„å¥–åŠ±ï¼Œç”¨äºè¯„ä¼°ç»¼è¿°çš„è´¨é‡ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒé˜¶æ®µä½œå¥–åŠ±å‡½æ•°ã€‚
- å¤šæ­¥å¼ºåŒ–å­¦ä¹ è®­ç»ƒç­–ç•¥ â€” æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä»¥ç¡®ä¿åœ¨ä¿ƒè¿›æœ‰æ•ˆæ¨ç†çš„åŒæ—¶ä¿ç•™å¿…è¦çš„ä¿¡æ¯ï¼Œå¹¶æ„å»ºäº†å¹¶è¡Œç¯å¢ƒï¼Œç»´æŒå¼ºåŒ–å­¦ä¹ è®­ç»ƒé«˜æ•ˆã€‚
##### ä½¿ç”¨ä¸æ¼”ç¤ºæ¡ˆä¾‹

è¯¦è§[æ­¤å¤„](./demo/minicpm4/SurveyGeneration/README.md)

##### è¯„ä¼°

| Method                                      | Relevance | Coverage | Depth | Novelty | Avg.  | Fact Score |
|---------------------------------------------|-----------|----------|-------|---------|-------|------------|
| Naive RAG (driven by G2FT)                  | 3.25      | 2.95     | 3.35  | 2.60    | 3.04  | 43.68      |
| AutoSurvey (driven by G2FT)                 | 3.10      | 3.25     | 3.15  | **3.15**| 3.16  | 46.56      |
| Webthinker (driven by WTR1-7B)              | 3.30      | 3.00     | 2.75  | 2.50    | 2.89  | --         |
| Webthinker (driven by QwQ-32B)              | 3.40      | 3.30     | 3.30  | 2.50    | 3.13  | --         |
| OpenAI Deep Research (driven by GPT-4o)     | 3.50      |**3.95**  | 3.55  | 3.00    | **3.50**  | --         |
| MiniCPM4-Survey                            | 3.45      | 3.70     | **3.85** | 3.00    | **3.50**  | **68.73**  |
| &nbsp;&nbsp;&nbsp;*w/o* RL                  | **3.55**  | 3.35     | 3.30  | 2.25    | 3.11  | 50.24      |

*GPT-4o å¯¹ç»¼è¿°ç”Ÿæˆç³»ç»Ÿçš„æ€§èƒ½æ¯”è¾ƒã€‚â€œG2FTâ€ ä»£è¡¨ Gemini-2.0-Flash-Thinkingï¼Œâ€œWTR1-7Bâ€ ä»£è¡¨ Webthinker-R1-7Bã€‚ç”±äº Webthinker ä¸åŒ…æ‹¬å¼•ç”¨åŠŸèƒ½ï¼ŒOpenAI Deep Research åœ¨å¯¼å‡ºç»“æœæ—¶ä¸æä¾›å¼•ç”¨ï¼Œå› æ­¤çœç•¥äº†å¯¹å®ƒä»¬çš„ FactScore è¯„ä¼°ã€‚æˆ‘ä»¬çš„æŠ€æœ¯æŠ¥å‘Šä¸­åŒ…å«è¯„æµ‹çš„è¯¦ç»†ä¿¡æ¯ã€‚*

#### MiniCPM4-MCP: MCPå¢å¼ºçš„å·¥å…·è°ƒç”¨

MiniCPM4-MCP æ˜¯ç”±[æ¸…åå¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤ï¼ˆTHUNLPï¼‰](https://nlp.csai.tsinghua.edu.cn)ã€ä¸­å›½äººæ°‘å¤§å­¦ä¸ [ModelBest](https://modelbest.cn/en) è”åˆå¼€å‘çš„å¼€æºæœ¬åœ°å¤§è¯­è¨€æ¨¡å‹ä»£ç†ï¼Œå®ƒåŸºäº MiniCPM-4-8Bï¼Œæ‹¥æœ‰ 80 äº¿å‚æ•°ã€‚å®ƒèƒ½å¤Ÿé€šè¿‡ MCP åè®®ä¸å„ç§å·¥å…·å’Œæ•°æ®èµ„æºäº¤äº’ï¼Œè§£å†³å¤šç§çœŸå®ä¸–ç•Œä»»åŠ¡ã€‚æˆªè‡³ç›®å‰ï¼ŒMiniCPM4-MCP å·²æ”¯æŒï¼š

- æ¶µç›– 16 ä¸ª MCP æœåŠ¡å™¨ï¼ˆserversï¼‰ä¸­å·¥å…·çš„ä½¿ç”¨ï¼šè¿™äº›æœåŠ¡å™¨æ‰€åŒ…å«çš„å·¥å…·æ¨ªè·¨äº†åŠå…¬ç±»ã€ç”Ÿæ´»ç±»ã€é€šè®¯ç±»ã€èµ„è®¯ç±»ã€å·¥ä½œç®¡ç†ç±»ç­‰.

- å•å·¥å…·ä½¿ç”¨çš„èƒ½åŠ›ï¼šå¯ä½¿ç”¨ç¬¦åˆ MCP åè®®çš„å·¥å…·è¿›è¡Œå•ä¸€å·¥å…·çš„ä¸€æ­¥æˆ–å¤šæ­¥è°ƒç”¨ã€‚

- è·¨å·¥å…·ç»„åˆä½¿ç”¨çš„èƒ½åŠ›ï¼šå¯ç»„åˆä½¿ç”¨ç¬¦åˆ MCP åè®®çš„ä¸åŒå·¥å…·ã€‚


##### ä½¿ç”¨ä¸æ¼”ç¤ºæ¡ˆä¾‹

è¯¦è§[æ­¤å¤„](./demo/minicpm4/MCP/README.md)

##### è¯„ä¼°

| MCP æœåŠ¡å™¨             |          | gpt-4o   |          |          | qwen3    |          |          | minicpm4 |          |
| -------------------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |
|                      | å‡½æ•°åæ­£ç¡®ç‡   | å‚æ•°åæ­£ç¡®ç‡    | æ•°å€¼æ­£ç¡®ç‡    | å‡½æ•°åæ­£ç¡®ç‡   | å‚æ•°åæ­£ç¡®ç‡    | æ•°å€¼æ­£ç¡®ç‡    | å‡½æ•°åæ­£ç¡®ç‡   | å‚æ•°åæ­£ç¡®ç‡    | æ•°å€¼æ­£ç¡®ç‡    |
| Airbnb                | 89.3           | 67.9         | 53.6         | 92.8          | 60.7         | 50.0         | 96.4           | 67.9         | 50.0         |
| Amap-Maps             | 79.8           | 77.5         | 50.0         | 74.4          | 72.0         | 41.0         | 89.3           | 85.7         | 39.9         |
| Arxiv-MCP-Server      | 85.7           | 85.7         | 85.7         | 81.8          | 54.5         | 50.0         | 57.1           | 57.1         | 52.4         |
| Calculator            | 100.0          | 100.0        | 20.0         | 80.0          | 80.0         | 13.3         | 100.0          | 100.0        | 6.67         |
| Computor-Control-MCP  | 90.0           | 90.0         | 90.0         | 90.0          | 90.0         | 90.0         | 90.0           | 90.0         | 86.7         |
| Desktop-Commander     | 100.0          | 100.0        | 100.0        | 100.0         | 100.0        | 100.0        | 100.0          | 100.0        | 100.0        |
| Filesystem            | 63.5           | 63.5         | 31.3         | 69.7          | 69.7         | 26.0         | 83.3           | 83.3         | 42.7         |
|Github | 92.0 | 80.0 | 58.0 | 80.5 | 50.0 | 27.7 | 62.8 | 25.7 | 17.1 |
| Gaode                 | 71.1           | 55.6         | 17.8         | 68.8          | 46.6         | 24.4         | 68.9           | 46.7         | 15.6         |
| MCP-Code-Executor     | 85.0           | 80.0         | 70.0         | 80.0          | 80.0         | 70.0         | 90.0           | 90.0         | 65.0         |
| MCP-Docx              | 95.8           | 86.7         | 67.1         | 94.9          | 81.6         | 60.1         | 95.1           | 86.6         | 76.1         |
| PPT                   | 72.6           | 49.8         | 40.9         | 85.9          | 50.7         | 37.5         | 91.2           | 72.1         | 56.7         |
| PPTx                  | 64.2           | 53.7         | 13.4         | 91.0          | 68.6         | 20.9         | 91.0           | 58.2         | 26.9         |
| Simple-Time-Server    | 90.0           | 70.0         | 70.0         | 90.0          | 90.0         | 90.0         | 90.0           | 60.0         | 60.0         |
| Slack                 | 100.0          | 90.0         | 70.0         | 100.0         | 100.0        | 65.0         | 100.0          | 100.0        | 100.0        |
| Whisper               | 90.0           | 90.0         | 90.0         | 90.0          | 90.0         | 90.0         | 90.0           | 90.0         | 30.0         |
| **å¹³å‡å€¼**              | **80.2**       | **70.2**     | **49.1**     | **83.5**      | **67.7**     | **43.8**     | **88.3**       | **76.1**     | **51.2**     |
  
### æ¨¡å‹æ¨ç†

#### CPM.cu

æˆ‘ä»¬æ¨èä½¿ç”¨ [CPM.cu](https://github.com/OpenBMB/cpm.cu) å¯¹ MiniCPM4 æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚CPM.cu æ˜¯é¢å£å¼€å‘çš„ä¸€ä¸ªé›†åˆäº†é«˜æ•ˆç¨€ç–ã€æŠ•æœºé‡‡æ ·ã€é‡åŒ–ç­‰æŠ€æœ¯çš„ CUDA æ¨ç†æ¡†æ¶ï¼Œèƒ½å¤Ÿå®Œå…¨å‘æŒ¥ MiniCPM4 çš„æ•ˆç‡ä¼˜åŠ¿ã€‚

ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹è„šæœ¬å®‰è£… CPM.cu å¹¶è¿›è¡Œæ¨ç†ï¼š

```bash
git clone https://github.com/OpenBMB/cpm.cu.git --recursive
cd cpm.cu
python3 setup.py install
```

MiniCPM4 åŸç”Ÿæ”¯æŒ 32,768 tokens çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚ä¸ºäº†å¤ç°è®ºæ–‡ä¸­çš„é•¿æ–‡æœ¬åŠ é€Ÿæ•ˆæœï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨æˆ‘ä»¬å·²éªŒè¯é€šè¿‡çš„LongRoPEå› å­ï¼Œä¿®æ”¹æ–¹æ³•ï¼šåœ¨`config.json`æ–‡ä»¶ä¸­è°ƒæ•´`rope_scaling`å­—æ®µå‚æ•°å³å¯ã€‚

```json
{
    ...,
    "rope_scaling": {
        "rope_type": "longrope", 
        "long_factor": [0.9977997200264581, 1.014658295992452, 1.0349680404997148, 1.059429246056193, 1.0888815016813513, 1.1243301355211495, 1.166977103606075, 1.2182568066927284, 1.2798772354275727, 1.3538666751582975, 1.4426259039919596, 1.5489853358570191, 1.6762658237220625, 1.8283407612492941, 2.0096956085876183, 2.225478927469756, 2.481536379650452, 2.784415934557119, 3.1413289096347365, 3.560047844772632, 4.048719380066383, 4.752651957515948, 5.590913044973868, 6.584005926629993, 7.7532214876576155, 9.119754865903639, 10.704443927019176, 12.524994176518703, 14.59739595363613, 16.93214476166354, 19.53823297353041, 22.417131025031697, 25.568260840911098, 28.991144156566317, 32.68408069090375, 36.65174474170465, 40.90396065611201, 45.4664008671033, 50.37147343433591, 55.6804490772103, 61.470816952306556, 67.8622707390618, 75.00516023410414, 83.11898235973767, 92.50044360202462, 103.57086856690864, 116.9492274587385, 118.16074567836519, 119.18497548708795, 120.04810876261652, 120.77352815196981, 121.38182790207875, 121.89094985353891, 122.31638758099915, 122.6714244963338, 122.9673822552567, 123.21386397019609, 123.41898278254268, 123.58957065488238, 123.73136519024158, 123.84917421274221, 123.94701903496814, 124.02825801299717, 124.09569231686116],
        "short_factor": [0.9977997200264581, 1.014658295992452, 1.0349680404997148, 1.059429246056193, 1.0888815016813513, 1.1243301355211495, 1.166977103606075, 1.2182568066927284, 1.2798772354275727, 1.3538666751582975, 1.4426259039919596, 1.5489853358570191, 1.6762658237220625, 1.8283407612492941, 2.0096956085876183, 2.225478927469756, 2.481536379650452, 2.784415934557119, 3.1413289096347365, 3.560047844772632, 4.048719380066383, 4.752651957515948, 5.590913044973868, 6.584005926629993, 7.7532214876576155, 9.119754865903639, 10.704443927019176, 12.524994176518703, 14.59739595363613, 16.93214476166354, 19.53823297353041, 22.417131025031697, 25.568260840911098, 28.991144156566317, 32.68408069090375, 36.65174474170465, 40.90396065611201, 45.4664008671033, 50.37147343433591, 55.6804490772103, 61.470816952306556, 67.8622707390618, 75.00516023410414, 83.11898235973767, 92.50044360202462, 103.57086856690864, 116.9492274587385, 118.16074567836519, 119.18497548708795, 120.04810876261652, 120.77352815196981, 121.38182790207875, 121.89094985353891, 122.31638758099915, 122.6714244963338, 122.9673822552567, 123.21386397019609, 123.41898278254268, 123.58957065488238, 123.73136519024158, 123.84917421274221, 123.94701903496814, 124.02825801299717, 124.09569231686116],
        "original_max_position_embeddings": 32768
    }
}
```

ä¿®æ”¹å®Œæˆåï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ï¼Œå¤ç°è®ºæ–‡ä¸­çš„é•¿æ–‡æœ¬åŠ é€Ÿæ•ˆæœ (è¿è¡Œè„šæœ¬ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹æƒé‡)

```bash
python3 tests/test_generate.py
```

æ›´å¤šå…³äº CPM.cu çš„ç»†èŠ‚ï¼Œè¯·å‚è€ƒ [CPM.cu ä»“åº“](https://github.com/OpenBMB/cpm.cu)ã€‚

#### HuggingFace

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(0)

path = 'openbmb/MiniCPM4-8B'
device = "cuda"
tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map=device, trust_remote_code=True)

# User can directly use the chat interface
# responds, history = model.chat(tokenizer, "Write an article about Artificial Intelligence.", temperature=0.7, top_p=0.7)
# print(responds)

# User can also use the generate interface
messages = [
    {"role": "user", "content": "Write an article about Artificial Intelligence."},
]
model_inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to(device)

model_outputs = model.generate(
    model_inputs,
    max_new_tokens=1024,
    top_p=0.7,
    temperature=0.7
)
output_token_ids = [
    model_outputs[i][len(model_inputs[i]):] for i in range(len(model_inputs))
]

responses = tokenizer.batch_decode(output_token_ids, skip_special_tokens=True)[0]
print(responses)
```

æœ¬æ¨¡å‹æ”¯æŒç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ InfLLM v2ï¼Œå¯é«˜æ•ˆå¤„ç†é•¿åºåˆ—æ¨ç†ã€‚å¦‚éœ€å¯ç”¨è¯¥åŠŸèƒ½ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–åº“ [infllmv2_cuda_impl](https://github.com/OpenBMB/infllmv2_cuda_impl)


è¿è¡Œä»¥ä¸‹å‘½ä»¤å³å¯å®‰è£…ï¼š

```bash
git clone -b feature_infer https://github.com/OpenBMB/infllmv2_cuda_impl.git
cd infllmv2_cuda_impl
git submodule update --init --recursive
pip install -e . # or python setup.py install 
```

å¯ç”¨ InfLLM v2 éœ€åœ¨ `config.json` é…ç½®æ–‡ä»¶ä¸­æ·»åŠ  `sparse_config` å­—æ®µï¼š

```json
{
    ...,
    "sparse_config": {
        "kernel_size": 32,
        "kernel_stride": 16,
        "init_blocks": 1,
        "block_size": 64,
        "window_size": 2048,
        "topk": 64,
        "use_nope": false,
        "dense_len": 8192
    }
}
```

è¿™äº›å‚æ•°æ§åˆ¶ InfLLM v2 çš„è¡Œä¸º:

* `kernel_size`ï¼ˆé»˜è®¤å€¼ï¼š32ï¼‰ï¼šè¯­ä¹‰æ ¸çš„å¤§å°ã€‚  
* `kernel_stride`ï¼ˆé»˜è®¤å€¼ï¼š16ï¼‰ï¼šç›¸é‚»è¯­ä¹‰æ ¸çš„æ­¥é•¿ã€‚  
* `init_blocks`ï¼ˆé»˜è®¤å€¼ï¼š1ï¼‰ï¼šæ¯ä¸ª query token å…³æ³¨çš„åˆå§‹çš„å—æ•°é‡ï¼Œç”¨äºç¡®ä¿å…³æ³¨åºåˆ—å¼€å¤´éƒ¨åˆ†ã€‚  
* `block_size`ï¼ˆé»˜è®¤å€¼ï¼š64ï¼‰ï¼škey-value blocks çš„å—å¤§å°ã€‚  
* `window_size`ï¼ˆé»˜è®¤å€¼ï¼š2048ï¼‰ï¼šå±€éƒ¨æ»‘åŠ¨çª—å£å¤§å°ã€‚  
* `topk`ï¼ˆé»˜è®¤å€¼ï¼š64ï¼‰ï¼šæ¯ä¸ª token ä»…ä¸æœ€ç›¸å…³çš„ top-k ä¸ª key-value blocks è®¡ç®—æ³¨æ„åŠ›ã€‚  
* `use_nope`ï¼ˆé»˜è®¤å€¼ï¼šfalseï¼‰ï¼šæ˜¯å¦åœ¨å—é€‰æ‹©ä¸­ä½¿ç”¨NOPEæŠ€æœ¯ä»¥æå‡æ€§èƒ½ã€‚  
* `dense_len`ï¼ˆé»˜è®¤å€¼ï¼š8192ï¼‰ï¼šç¨€ç–æ³¨æ„åŠ›å¯¹çŸ­åºåˆ—æ”¶ç›Šæœ‰é™ï¼Œå½“ token é•¿åº¦ä½äºæ­¤é˜ˆå€¼æ—¶è‡ªåŠ¨åˆ‡æ¢ä¸ºæ ‡å‡†æ³¨æ„åŠ›ã€‚è®¾ä¸º `-1` åˆ™å¼ºåˆ¶å§‹ç»ˆä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›ã€‚

Minicpm4 åŸç”Ÿæ”¯æŒ 32,768 tokens çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚è‹¥å¯¹è¯æ€»é•¿åº¦ï¼ˆè¾“å…¥ + è¾“å‡ºï¼‰è¿œè¶…æ­¤é™åˆ¶ï¼Œå»ºè®®é€šè¿‡ RoPE ç¼©æ”¾æŠ€æœ¯æ‰©å±•ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬å·²éªŒè¯é€šè¿‡è°ƒæ•´ LongRoPE å› å­ï¼Œæ¨¡å‹å¯ç¨³å®šæ”¯æŒ 131,072 tokens çš„è¶…é•¿ä¸Šä¸‹æ–‡ã€‚

ä¿®æ”¹æ–¹æ³•ï¼šåœ¨ `config.json` æ–‡ä»¶ä¸­è°ƒæ•´ `rope_scaling` å­—æ®µå‚æ•°å³å¯ã€‚

```json
{
    ...,
    "rope_scaling": {
        "rope_type": "longrope", 
        "long_factor": [0.9977997200264581, 1.014658295992452, 1.0349680404997148, 1.059429246056193, 1.0888815016813513, 1.1243301355211495, 1.166977103606075, 1.2182568066927284, 1.2798772354275727, 1.3538666751582975, 1.4426259039919596, 1.5489853358570191, 1.6762658237220625, 1.8283407612492941, 2.0096956085876183, 2.225478927469756, 2.481536379650452, 2.784415934557119, 3.1413289096347365, 3.560047844772632, 4.048719380066383, 4.752651957515948, 5.590913044973868, 6.584005926629993, 7.7532214876576155, 9.119754865903639, 10.704443927019176, 12.524994176518703, 14.59739595363613, 16.93214476166354, 19.53823297353041, 22.417131025031697, 25.568260840911098, 28.991144156566317, 32.68408069090375, 36.65174474170465, 40.90396065611201, 45.4664008671033, 50.37147343433591, 55.6804490772103, 61.470816952306556, 67.8622707390618, 75.00516023410414, 83.11898235973767, 92.50044360202462, 103.57086856690864, 116.9492274587385, 118.16074567836519, 119.18497548708795, 120.04810876261652, 120.77352815196981, 121.38182790207875, 121.89094985353891, 122.31638758099915, 122.6714244963338, 122.9673822552567, 123.21386397019609, 123.41898278254268, 123.58957065488238, 123.73136519024158, 123.84917421274221, 123.94701903496814, 124.02825801299717, 124.09569231686116],
        "short_factor": [0.9977997200264581, 1.014658295992452, 1.0349680404997148, 1.059429246056193, 1.0888815016813513, 1.1243301355211495, 1.166977103606075, 1.2182568066927284, 1.2798772354275727, 1.3538666751582975, 1.4426259039919596, 1.5489853358570191, 1.6762658237220625, 1.8283407612492941, 2.0096956085876183, 2.225478927469756, 2.481536379650452, 2.784415934557119, 3.1413289096347365, 3.560047844772632, 4.048719380066383, 4.752651957515948, 5.590913044973868, 6.584005926629993, 7.7532214876576155, 9.119754865903639, 10.704443927019176, 12.524994176518703, 14.59739595363613, 16.93214476166354, 19.53823297353041, 22.417131025031697, 25.568260840911098, 28.991144156566317, 32.68408069090375, 36.65174474170465, 40.90396065611201, 45.4664008671033, 50.37147343433591, 55.6804490772103, 61.470816952306556, 67.8622707390618, 75.00516023410414, 83.11898235973767, 92.50044360202462, 103.57086856690864, 116.9492274587385, 118.16074567836519, 119.18497548708795, 120.04810876261652, 120.77352815196981, 121.38182790207875, 121.89094985353891, 122.31638758099915, 122.6714244963338, 122.9673822552567, 123.21386397019609, 123.41898278254268, 123.58957065488238, 123.73136519024158, 123.84917421274221, 123.94701903496814, 124.02825801299717, 124.09569231686116],
        "original_max_position_embeddings": 32768
    }
}
```

#### vLLM
* å®‰è£…
  
å‚ç…§ vLLM [å®˜æ–¹ä»“åº“](https://github.com/vllm-project/vllm)ï¼Œé€šè¿‡*æºç *å®‰è£…æœ€æ–°ç‰ˆæœ¬ã€‚
```
pip install -U vllm \
    --pre \
    --extra-index-url https://wheels.vllm.ai/nightly
```

* ä½¿ç”¨ vLLM æ¨ç† MiniCPM4-8B æ¨¡å‹ï¼š
```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

model_name = "openbmb/MiniCPM4-8B"
prompt = [{"role": "user", "content": "æ¨è5ä¸ªåŒ—äº¬çš„æ™¯ç‚¹ã€‚"}]

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
input_text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)

llm = LLM(
    model=model_name,
    trust_remote_code=True,
    max_num_batched_tokens=32768, 
    dtype="bfloat16", 
    gpu_memory_utilization=0.8, 
)
sampling_params = SamplingParams(top_p=0.7, temperature=0.7, max_tokens=1024, repetition_penalty=1.02)

outputs = llm.generate(prompts=input_text, sampling_params=sampling_params)

print(outputs[0].outputs[0].text)
```

* åœ¨ vLLM ä¸­ä½¿ç”¨ Eagle æŠ•æœºè§£ç ï¼šåªéœ€å¦‚ä¸‹åˆå§‹åŒ–æ¨ç†å¼•æ“
```python
llm = LLM(
    model=model_name,
    trust_remote_code=True,
    max_num_batched_tokens=32768, 
    dtype="bfloat16", 
    gpu_memory_utilization=0.8, 
    speculative_config={
        "method": "eagle",
        "model": "openbmb/MiniCPM4-8B-Eagle-vLLM",
        "num_speculative_tokens": 2,
        "max_model_len": 32768,
    },
)
```

* åœ¨ vLLM ä¸­æ¨ç†é‡åŒ–åçš„ MiniCPM4-8Bï¼šåªéœ€å¦‚ä¸‹åˆå§‹åŒ–æ¨ç†å¼•æ“
```python
llm = LLM(
    model="openbmb/MiniCPM4-8B-marlin-Eagle-vLLM",
    trust_remote_code=True,
    max_num_batched_tokens=32768, 
    dtype="bfloat16", 
    gpu_memory_utilization=0.8, 
)
```

* åœ¨ vLLM ä¸­ä½¿ç”¨ Eagle æŠ•æœºè§£ç æ¨ç†é‡åŒ–åçš„ MiniCPM4-8Bï¼šåªéœ€å¦‚ä¸‹åˆå§‹åŒ–æ¨ç†å¼•æ“
```python
llm = LLM(
    model="openbmb/MiniCPM4-8B-marlin-Eagle-vLLM",
    trust_remote_code=True,
    max_num_batched_tokens=32768, 
    dtype="bfloat16", 
    gpu_memory_utilization=0.8, 
    speculative_config={
        "method": "eagle",
        "model": "openbmb/MiniCPM4-8B-marlin-vLLM",
        "num_speculative_tokens": 2,
        "max_model_len": 32768,
    },
)
```

#### SGLang
* å®‰è£…

å‚è€ƒ SGLang [å®˜æ–¹ä»“åº“](ttps://github.com/sgl-project/sglang)ï¼Œé€šè¿‡*æºç *å®‰è£…ã€‚
```
git clone -b openbmb https://github.com/OpenBMB/sglang.git
cd sglang

pip install --upgrade pip
pip install -e "python[all]"
```

* å¯åŠ¨æ¨ç†æœåŠ¡
```shell
python -m sglang.launch_server --model openbmb/MiniCPM4-8B --trust-remote-code --port 30000 --chat-template chatml
```

* ç„¶åç”¨æˆ·å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥ä½¿ç”¨èŠå¤©ç•Œé¢ï¼š
```python
import openai

client = openai.Client(base_url=f"http://localhost:30000/v1", api_key="None")

response = client.chat.completions.create(
    model="openbmb/MiniCPM4-8B",
    messages=[
        {"role": "user", "content": "Write an article about Artificial Intelligence."},
    ],
    temperature=0.7,
    max_tokens=1024,
)
print(response.choices[0].message.content)
```

* ä½¿ç”¨æŠ•æœºåŠ é€Ÿ
```shell
python3 -m sglang.launch_server --model-path [model] \ 
    --speculative_draft_model_path [draft_model] \
    --host 0.0.0.0 --trust-remote-code \
    --speculative-algorithm EAGLE --speculative-num-steps 1 --speculative-eagle-topk 1 --speculative-num-draft-tokens 2 \
    --mem-fraction 0.5
```

## MiniCPM 3.0
<details>
<summary>æŸ¥çœ‹ MiniCPM 3.0 çš„è¯¦ç»†ä¿¡æ¯</summary>

MiniCPM 3.0 æ˜¯ä¸€ä¸ª 4B å‚æ•°é‡çš„è¯­è¨€æ¨¡å‹ï¼Œç›¸æ¯” MiniCPM1.0/2.0ï¼ŒåŠŸèƒ½æ›´åŠ å…¨é¢ï¼Œç»¼åˆèƒ½åŠ›å¤§å¹…æå‡ï¼Œå¤šæ•°è¯„æµ‹é›†ä¸Šçš„æ•ˆæœæ¯”è‚©ç”šè‡³è¶…è¶Šä¼—å¤š 7B-9B æ¨¡å‹ã€‚
* **æ”¯æŒå·¥å…·è°ƒç”¨ğŸ› ï¸ï¼ˆFunction Callingï¼‰å’Œä»£ç è§£é‡Šå™¨ğŸ’»ï¼ˆCode Interpreterï¼‰**ï¼š[Berkeley Function Calling Leaderboard (BFCL)](https://gorilla.cs.berkeley.edu/leaderboard.html) ä¸Šå–å¾— 9B è§„æ¨¡ä»¥ä¸‹ SOTAï¼Œè¶…è¶Š GLM-4-9B-Chatã€Qwen2-7B-Instructã€‚
* **è¶…å¼ºçš„æ¨ç†èƒ½åŠ›ğŸ§®**ï¼šæ•°å­¦èƒ½åŠ›æ–¹é¢ï¼Œ[MathBench](https://open-compass.github.io/MathBench/) ä¸Šçš„æ•ˆæœè¶…è¶Š GPT-3.5-Turbo ä»¥åŠå¤šä¸ª 7B-9B æ¨¡å‹ã€‚åœ¨éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ [LiveCodeBench](https://livecodebench.github.io/) ä¸Šï¼Œæ•ˆæœè¶…è¶Š Llama3.1-8B-Instructã€‚
* **å‡ºè‰²çš„ä¸­è‹±æ–‡æŒ‡ä»¤éµå¾ªèƒ½åŠ›ğŸ¤–**ï¼šè‹±æ–‡æŒ‡ä»¤éµå¾ª [IFEval](https://huggingface.co/datasets/google/IFEval)ã€ä¸­æ–‡æŒ‡ä»¤éµå¾ª [FollowBench-zh](https://huggingface.co/datasets/YuxinJiang/FollowBench) æ•ˆæœè¶…è¶Š GLM-4-9B-Chatã€Qwen2-7B-Instructã€‚
* **é•¿æ–‡æœ¬èƒ½åŠ›**ï¼šåŸç”Ÿæ”¯æŒ 32k ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ32k é•¿åº¦å†…å¤§æµ·æé’ˆå…¨ç»¿ã€‚æå‡º [LLMxMapReduce](https://github.com/thunlp/LLMxMapReduce) ï¼Œç†è®ºå¯å¤„ç†çš„ä¸Šä¸‹æ–‡é•¿åº¦è¾¾åˆ° +âˆï¼Œåœ¨ç»¼åˆæ€§é•¿æ–‡æœ¬è¯„æµ‹åŸºå‡† [InfiniteBench](https://github.com/OpenBMB/InfiniteBench) å¹³å‡å¾—åˆ†è¶…è¶ŠGPT-4ã€KimiChatç­‰æ ‡æ†æ¨¡å‹ã€‚
* **RAGèƒ½åŠ›**ï¼šæˆ‘ä»¬å‘å¸ƒäº† [MiniCPM RAG å¥—ä»¶](https://huggingface.co/collections/openbmb/minicpm-rag-suite-66d976b4204cd0a4f8beaabb)ã€‚åŸºäº MiniCPM ç³»åˆ—æ¨¡å‹çš„ [MiniCPM-Embedding](https://huggingface.co/openbmb/MiniCPM-Embedding)ã€[MiniCPM-Reranker](https://huggingface.co/openbmb/MiniCPM-Reranker) åœ¨ä¸­æ–‡ã€ä¸­è‹±è·¨è¯­è¨€æ£€ç´¢æµ‹è¯•ä¸­å–å¾— SOTA è¡¨ç°ï¼›é’ˆå¯¹ RAG åœºæ™¯çš„ [MiniCPM3-RAG-LoRA](https://huggingface.co/openbmb/MiniCPM3-RAG-LoRA) åœ¨å¼€æ”¾åŸŸé—®ç­”ç­‰å¤šé¡¹ä»»åŠ¡ä¸Šè¶…è¶Š Llama3-8Bã€Baichuan2-13B ç­‰æ¨¡å‹ã€‚

### è¯„æµ‹ç»“æœ

#### ç»¼åˆè¯„æµ‹

<table>
    <tr>
        <td>è¯„æµ‹é›†</td>
        <td>Qwen2-7B-Instruct</td>
        <td>GLM-4-9B-Chat</td>
        <td>Gemma2-9B-it</td>
        <td>Llama3.1-8B-Instruct</td>
        <td>GPT-3.5-Turbo-0125</td>
        <td>Phi-3.5-mini-Instruct(3.8B)</td>
        <td>MiniCPM3-4B </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>è‹±æ–‡èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>MMLU</td>
        <td>70.5</td>
        <td>72.4</td>
        <td>72.6</td>
        <td>69.4</td>
        <td>69.2</td>
        <td>68.4</td>
        <td>67.2 </td>
    </tr>
    <tr>
        <td>BBH</td>
        <td>64.9</td>
        <td>76.3</td>
        <td>65.2</td>
        <td>67.8</td>
        <td>70.3</td>
        <td>68.6</td>
        <td>70.2 </td>
    </tr>
    <tr>
        <td>MT-Bench</td>
        <td>8.41</td>
        <td>8.35</td>
        <td>7.88</td>
        <td>8.28</td>
        <td>8.17</td>
        <td>8.60</td>
        <td>8.41 </td>
    </tr>
    <tr>
        <td>IFEVAL (Prompt Strict-Acc.)</td>
        <td>51.0</td>
        <td>64.5</td>
        <td>71.9</td>
        <td>71.5</td>
        <td>58.8</td>
        <td>49.4</td>
        <td>68.4 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>ä¸­æ–‡èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>CMMLU</td>
        <td>80.9</td>
        <td>71.5</td>
        <td>59.5</td>
        <td>55.8</td>
        <td>54.5</td>
        <td>46.9</td>
        <td>73.3 </td>
    </tr>
    <tr>
        <td>CEVAL</td>
        <td>77.2</td>
        <td>75.6</td>
        <td>56.7</td>
        <td>55.2</td>
        <td>52.8</td>
        <td>46.1</td>
        <td>73.6 </td>
    </tr>
    <tr>
        <td>AlignBench v1.1</td>
        <td>7.10</td>
        <td>6.61</td>
        <td>7.10</td>
        <td>5.68</td>
        <td>5.82</td>
        <td>5.73</td>
        <td>6.74 </td>
    </tr>
    <tr>
        <td>FollowBench-zh (SSR)</td>
        <td>63.0</td>
        <td>56.4</td>
        <td>57.0</td>
        <td>50.6</td>
        <td>64.6</td>
        <td>58.1</td>
        <td>66.8 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>æ•°å­¦èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>MATH</td>
        <td>49.6</td>
        <td>50.6</td>
        <td>46.0</td>
        <td>51.9</td>
        <td>41.8</td>
        <td>46.4</td>
        <td>46.6 </td>
    </tr>
    <tr>
        <td>GSM8K</td>
        <td>82.3</td>
        <td>79.6</td>
        <td>79.7</td>
        <td>84.5</td>
        <td>76.4</td>
        <td>82.7</td>
        <td>81.1 </td>
    </tr>
    <tr>
        <td>MathBench</td>
        <td>63.4</td>
        <td>59.4</td>
        <td>45.8</td>
        <td>54.3</td>
        <td>48.9</td>
        <td>54.9</td>
        <td>65.6 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>ä»£ç èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>HumanEval+</td>
        <td>70.1</td>
        <td>67.1</td>
        <td>61.6</td>
        <td>62.8</td>
        <td>66.5</td>
        <td>68.9</td>
        <td>68.3 </td>
    </tr>
    <tr>
        <td>MBPP+</td>
        <td>57.1</td>
        <td>62.2</td>
        <td>64.3</td>
        <td>55.3</td>
        <td>71.4</td>
        <td>55.8</td>
        <td>63.2 </td>
    </tr>
    <tr>
        <td>LiveCodeBench v3</td>
        <td>22.2</td>
        <td>20.2</td>
        <td>19.2</td>
        <td>20.4</td>
        <td>24.0</td>
        <td>19.6</td>
        <td>22.6 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>å·¥å…·è°ƒç”¨èƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>BFCL v2</td>
        <td>71.6</td>
        <td>70.1</td>
        <td>19.2</td>
        <td>73.3</td>
        <td>75.4</td>
        <td>48.4</td>
        <td>76.0 </td>
    </tr>
    <tr>
        <td colspan="15" align="left"><strong>ç»¼åˆèƒ½åŠ›</strong></td>
    </tr>
    <tr>
        <td>å¹³å‡åˆ†</td>
        <td>65.3</td>
        <td>65.0</td>
        <td>57.9</td>
        <td>60.8</td>
        <td>61.0</td>
        <td>57.2</td>
        <td><strong>66.3</strong></td>
    </tr>
</table>

#### å·¥å…·è°ƒç”¨èƒ½åŠ›

æˆ‘ä»¬åœ¨ [Berkeley Function Calling Leaderboard (BFCL)](https://gorilla.cs.berkeley.edu/leaderboard.html) ä¸Šæµ‹è¯•äº†æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼ŒMiniCPM3-4B åœ¨è¯¥æ¦œå•ä¸Šçš„è¡¨ç°è¶…è¶Šäº†å¤šä¸ª 7B-9B å‚æ•°é‡çš„æ¨¡å‹ï¼Œä¼˜äº GPT-3.5-Turbo-0125ã€‚

<table>
    <tr>
        <td>æ¨¡å‹</td>
        <td>æ€»ä½“å‡†ç¡®ç‡</td>
        <td>AST Summary</td>
        <td>Exec Summary</td>
        <td>Irrelevance Detection</td>
        <td>Relevance Detection </td>
    </tr>
    <tr>
        <td>MiniCPM3-4B</td>
        <td>76.03%</td>
        <td>68.55%</td>
        <td>85.54%</td>
        <td>53.71%</td>
        <td>90.24% </td>
    </tr>
    <tr>
        <td>Llama3.1-8B-Instruct</td>
        <td>73.28%</td>
        <td>64.61%</td>
        <td>86.48%</td>
        <td>43.12%</td>
        <td>85.37% </td>
    </tr>
    <tr>
        <td>Qwen2-7B-Instruct</td>
        <td>71.61%</td>
        <td>65.71%</td>
        <td>79.57%</td>
        <td>44.70%</td>
        <td>90.24% </td>
    </tr>
    <tr>
        <td>GLM-4-9B-Chat</td>
        <td>70.08%</td>
        <td>60.69%</td>
        <td>80.02%</td>
        <td>55.02%</td>
        <td>82.93% </td>
    </tr>
    <tr>
        <td>Phi-3.5-mini-instruct</td>
        <td>48.44%</td>
        <td>38.89%</td>
        <td>54.04%</td>
        <td>46.78%</td>
        <td>65.85% </td>
    </tr>
    <tr>
        <td>Gemma2-9B-it</td>
        <td>19.18%</td>
        <td>5.41%</td>
        <td>18.50%</td>
        <td>88.88%</td>
        <td>7.32%</td>
    </tr>
</table>

#### é•¿æ–‡æœ¬èƒ½åŠ›

åœ¨ 32k çš„ä¸Šä¸‹æ–‡é•¿åº¦è¿›è¡Œ[å¤§æµ·æé’ˆ](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)æµ‹è¯•ï¼Œç»“æœå¦‚ä¸‹å›¾ï¼š

![needle](assets/minicpm3/eval_needle.jpeg)

åŒæ—¶æˆ‘ä»¬æå‡º[LLMxMapReduce](https://github.com/thunlp/LLMxMapReduce)ï¼Œåˆ©ç”¨åˆ†æ²»çš„ç­–ç•¥ï¼Œç†è®ºä¸Šå¯ä»¥å¤„ç†æ— é™é•¿åº¦çš„æ–‡æœ¬ã€‚æˆ‘ä»¬åœ¨[InfiniteBench](https://github.com/OpenBMB/InfiniteBench)ä¸Šæµ‹è¯•äº†æ¨¡å‹çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ï¼Œåœ¨LLMxMapReduceæ¡†æ¶çš„åŠ æŒä¸‹ï¼ŒMiniCPM3-4Båœ¨è¿™ä¸ªæ¦œå•çš„å¹³å‡å¾—åˆ†èƒ½å¤Ÿè¶…è¶Š GPT-4ã€KimiChat ç­‰æ ‡æ†æ¨¡å‹ã€‚

|                               | Context length| Qwen2-70b | Kimi-Chat(2024.06) | GPT-4 (From InfiniteBench) | MiniCPM 3.0 x MR | Qwen2-70b x MR | Llama3-70bx MR |
| ----------------------------- | ---------- | --------- | ------------------ | -------------------------- | --------------- | ------------ | ------------- |
| Math.Find                     | 87.9k      | 59.71%    | 18.57%             | 60.00%                     | 83.43%          | 54.29%       | **91.43%**        |
| Retrieve.KV                   | 89.9k      | 29.00%    | 69.20%             | 89.00%                     | 93.80%          | 98.80%       | **98.89%**        |
| En.Dia                        | 103.6K     | 23.00%    | 23.00%             | 7.50%                      | 12.50%          | **46.50%**       | 17.50%        |
| Code.Debug                    | 114.7k     | 45.43%    | 38.32%             | 54.31%                     | 25.63%          | 54.82%       | **62.94%**       |
| Retrieve.Number               | 122.4k     | **100.00%**  | 97.45%             | **100.00%**                   | 99.32%          | **100.00%**     | 99.79%        |
| Retrieve.PassKey              | 122.4k     | **100.00%**   | 99.32%             | **100.00%**                   | 98.81%          | **100.00%**     | **100.00%**      |
| En.Sum                        | 171.5K     | 31.85%    | 29.94%             | 14.73%                     | 25.89%          | **32.39%**       | 30.63%        |
| En.MC                         | 184.4k     | 81.66%    | 79.91%             | 68.12%                     | 66.38%          |**83.84%**      | 82.10%        |
| En.QA        | 192.6k     | 21.97%    | 18.80%             | 22.44%                     | 28.39%          | 23.13%       | **34.70%**      |
| Zh.QA        | 2068.6k    | 21.40%    | 19.84%             | **25.96%**                    | 23.66%          | 19.10%       | N/A           |
| avg w/o Zh.QA | /          | 51.92%    | 52.96%             | 55.33%                     | 59.29%          | 64.98%       | **68.64%**        |
| avg                           | /          | 48.86%    | 49.65%             | 52.39%                     | 55.55%          | **60.39%**       | N/A           |

### æ¨¡å‹æ¨ç†

#### Huggingface
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(0)

path = 'openbmb/MiniCPM3-4B'
tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)

responds, history = model.chat(tokenizer, "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼Œè¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•å’Œéšæ‚£ã€‚", temperature=0.7, top_p=0.7)
print(responds)
```

#### SGLangï¼ˆæ¨èï¼‰
* å®‰è£…

å‚è€ƒ SGLang [å®˜æ–¹ä»“åº“](ttps://github.com/sgl-project/sglang)ï¼Œé€šè¿‡*æºç *å®‰è£…æœ€æ–°ç‰ˆæœ¬ã€‚

* å¯åŠ¨æ¨ç†æœåŠ¡
```shell
python -m sglang.launch_server --model openbmb/MiniCPM3-4B --trust-remote-code --port 30000 --chat-template chatml
```

* ä½¿ç”¨ç¤ºä¾‹
```python
from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint

@function
def multi_turn_question(s, question_1, question_2):
    s += user(question_1)
    s += assistant(gen("answer_1", max_tokens=1024))
    s += user(question_2)
    s += assistant(gen("answer_2", max_tokens=1024))

set_default_backend(RuntimeEndpoint("http://localhost:30000"))

state = multi_turn_question.run(
    question_1="ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
    question_2="å†™ä¸€ç¯‡å…³äºå®ƒçš„æ–‡ç« ",
)

for m in state.messages():
    print(m["role"], ":", m["content"])
```

#### vLLM
* å®‰è£… vllm
  ```shell
  pip install "vllm>=0.6.2"
  ```
* æ¨ç†
  ```python
  from transformers import AutoTokenizer
  from vllm import LLM, SamplingParams

  model_name = "openbmb/MiniCPM3-4B"
  prompt = [{"role": "user", "content": "è¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼Œè¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•å’Œéšæ‚£ã€‚"}]

  tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  input_text = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)

  llm = LLM(model=model_name,
      trust_remote_code=True,
      tensor_parallel_size=1
  )
  sampling_params = SamplingParams(top_p=0.7, temperature=0.7, max_tokens=1024)

  outputs = llm.generate(prompts=input_text, sampling_params=sampling_params)

  print(outputs[0].outputs[0].text)
  ```

#### llama.cpp

æˆ‘ä»¬æä¾›äº† MiniCPM3 çš„ [GGUF ç‰ˆæœ¬](https://huggingface.co/openbmb/MiniCPM3-4B-GGUF)ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ llama.cpp æ¨ç†ã€‚

* å®‰è£… llama.cpp
  ```shell
    git clone https://github.com/ggerganov/llama.cpp
    cd llama.cpp
    make 
  ```
* æ¨ç†
  ```shell
  ./llama-cli -c 1024 -m minicpm3-4b-fp16.gguf -n 1024 --top-p 0.7 --temp 0.7 --prompt "<|im_start|>user\nè¯·å†™ä¸€ç¯‡å…³äºäººå·¥æ™ºèƒ½çš„æ–‡ç« ï¼Œè¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•å’Œéšæ‚£ã€‚<|im_end|>\n<|im_start|>assistant\n"
  ```

### æ¨¡å‹å¾®è°ƒ
#### LLaMA-Factory
ç›®å‰æ¨¡å‹å¾®è°ƒæ”¯æŒ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)ï¼Œä½¿ç”¨æ–¹æ³•å‚è€ƒ [LLaMA-Factory å¾®è°ƒ](https://modelbest.feishu.cn/docx/Z7USdW4lloZzkZxQ14icJ3senjb?from=from_copylink)ã€‚

### è¿›é˜¶åŠŸèƒ½

å¯¹äºä»¥ä¸‹è¿›é˜¶åŠŸèƒ½ï¼Œæˆ‘ä»¬çš„æ ·ä¾‹ä»£ç ä¸­ä½¿ç”¨ [vLLM](#vllm) è¿›è¡Œæ¨ç†ã€‚

#### å·¥å…·è°ƒç”¨

æˆ‘ä»¬æä¾›äº†ä½¿ç”¨ MiniCPM3 è°ƒç”¨å·¥å…·çš„ç¤ºä¾‹ä»£ç ï¼š

```bash
cd demo/minicpm3/function_call
python function_call.py
```

å¦‚æœä½ æƒ³å¯åŠ¨ä¸€ä¸ªèƒ½å¤Ÿè°ƒç”¨å·¥å…·çš„æ¨ç†æœåŠ¡ï¼Œä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š

```bash
cd demo/minicpm3/function_call
pip install -r requirements.txt
python openai_api_server.py \
    --model openbmb/MiniCPM3-4B \
    --served-model-name MiniCPM3-4B \
    --chat-template chatml.jinja \
    --dtype auto \
    --api-key token-abc123 \
    --tensor-parallel-size 1 \
    --trust-remote-code
```

ä¸‹é¢æ˜¯ä¸€ä¸ªè°ƒç”¨æœç´¢å·¥å…·å›ç­”é—®é¢˜çš„æ¼”ç¤ºï¼š

![function_call](./assets/minicpm3/function_call.gif)

#### ä»£ç è§£é‡Šå™¨

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª MiniCPM3 ä½¿ç”¨ä»£ç è§£é‡Šå™¨çš„ç¤ºä¾‹ä»£ç ï¼š

```bash
cd demo/minicpm3/code_interpreter
pip install -r requirements.txt
python code_interpreter.py openbmb/MiniCPM3-4B
```

ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ä»£ç è§£é‡Šå™¨ç”ŸæˆäºŒç»´ç çš„æ¼”ç¤ºï¼š

![code_interpreter](./assets/minicpm3/code_interpreter.gif)
</details>

## MiniCPM 2.0

<details>
<summary>æŸ¥çœ‹ MiniCPM 2.0 çš„è¯¦ç»†ä¿¡æ¯</summary>

MiniCPM 2.0 ç³»åˆ—æ¨¡å‹å¯¹ MiniCPM è¿›è¡Œäº†å¤šä¸ªç»´åº¦çš„å‡çº§ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ¨¡å‹ç‰ˆæœ¬ï¼š
- MiniCPM-2B-128kï¼šå°† MiniCPM-2B çš„ä¸Šä¸‹æ–‡é•¿åº¦ä» 4k æ‰©å±•è‡³ 128kï¼Œåœ¨ InfiniteBench æµ‹è¯•é›†ä¸Šä¼˜äº ChatGLM3-6B-128kã€Yi-6B-200k ç­‰æ›´å¤§å‚æ•°é‡çš„æ¨¡å‹ã€‚
- MiniCPM-MoE-8x2Bï¼šåŸºäº MiniCPM-2B è¿›è¡Œ MoE æ‰©å±•ï¼Œç»¼åˆè¡¨ç°ç›¸æ¯”äº MiniCPM-2B å¹³å‡æé«˜ 4.5 ä¸ªç™¾åˆ†ç‚¹ã€‚
- MiniCPM-1Bï¼šç›¸æ¯”äº MiniCPM-2B æˆæœ¬ä¸‹é™ 60%ï¼Œç»¼åˆè¡¨ç°ä»ç„¶ä¼˜äº LLaMA2-13Bã€‚
- MiniCPM-S-1Bï¼šåœ¨ä¿æŒä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½æ— æŸçš„å‰æä¸‹ï¼ŒFFN å±‚å®ç°äº† 87.89% çš„å¹³å‡ç¨€ç–åº¦ï¼Œå°† FFN FLOPs é™ä½äº† 84%ã€‚ç»“åˆ PowerInfer æ¨ç†æ¡†æ¶ï¼Œè§£ç é€Ÿåº¦æå‡çº¦ 2.8 å€ã€‚

### è¯„æµ‹ç»“æœ

#### MiniCPM-2B-128k æ¨¡å‹è¯„æµ‹
| Model                               | avg   | avg w/o code&math | passkey | number_string | kv_retrieval | longbook_choice_eng | longbook_qa_chn | longbook_qa_eng | longbook_sum_eng | longdialogue_qa_eng | math_calc | math_find | code_debug | code_run |
|-------------------------------------|-------|-------------------|---------|---------------|--------------|---------------------|-----------------|-----------------|------------------|---------------------|-----------|-----------|------------|----------|
| LWM-Text-128k                       | 24.45 | 33.62             | 100     | 97.8          | 0.6          | 28.82               | 15.93           | 14.31           | 9.99             | 1.5                 | 0         | 3.43      | 20.05      | 1        |
| Yarn-Mistral-7b-128k                | 19.84 | 27.36             | 92.71   |               | 0            | 27.95               | 15.49           | 9.55            | 9.06             | 7.5                 | 0         | 17.14     | 0.76       | 1.25     |
| Mistral-7B-Instruct-v0.2(ABF 1000w) | 27.75 | 36.9              | 100     | 78.98         | 3.6          | 37.12               | 11.74           | 17.37           | 21.12            | 9.5                 | 0         | 29.43     | 17.51      | 0        |
| Yi-6B-200k                          | 22.15 | 32.54             | 100     | 94.92         | 0            | 36.68               | 15.07           | 9.2             | 0.92             | 3.5                 | 0         | 4.29      | 0.51       | 0.75     |
| chatglm3-6b-128k                    | 25.58 | 36.57             | 89.93   | 99.66         | 5.2          | 46.29               | 10.7            | 8.38            | 25.91            | 6.5                 | 0         | 8         | 5.33       | 1        |
| MiniCPM-2.4B-128k                   | 27.32 | 37.68             | 98.31   | 99.83         | 9            | 29.69               | 23.06           | 16.33           | 15.73            | 9.5                 | 0         | 4.29      | 22.08      | 0        |

#### MiniCPM-MoE-8x2B æ¨¡å‹è¯„æµ‹
<div align="left">

<table style="margin: 0px auto;">
<thead>
  <tr>
    <th align="left">Model</th>
    <th nowrap="nowrap" >BBH</th>
    <th nowrap="nowrap" >MMLU</th>
    <th nowrap="nowrap" >CEval</th>
    <th nowrap="nowrap" >CMMLU</th>
    <th nowrap="nowrap" >HumanEval</th>
    <th nowrap="nowrap" >MBPP&dagger;</th>
    <th nowrap="nowrap" >GSM8K</th>
    <th nowrap="nowrap" >MATH</th
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td nowrap="nowrap" align="left">Llama2-34B*</td>
    <td>44.1</td>
    <td>62.6</td>
    <td>-</td>
    <td>-</td>
    <td>22.6</td>
    <td>33.0</td>
    <td>42.2</td>
    <td>6.24</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left">Mistral-7B-Instruct-v0.2</td>
    <td>39.81</td>
    <td>60.51</td>
    <td>42.55</td>
    <td>41.92</td>
    <td>36.59</td>
    <td>39.63</td>
    <td>40.49</td>
    <td>4.95</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" >Gemma-7B*</td>
    <td>55.1</td>
    <td>64.3</td>
    <td>-</td>
    <td>-</td>
    <td>32.3</td>
    <td>44.4</td>
    <td>46.4</td>
    <td>24.3</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" >Qwen1.5-7B*</td>
    <td>40.2</td>
    <td>61</td>
    <td>74.1</td>
    <td>73.1</td>
    <td>36</td>
    <td>37.4</td>
    <td>62.5</td>
    <td>20.3</td>
  </tr>
  <tr>
    <td  nowrap="nowrap" align="left" >Deepseek-MoE(16B)*</td>
    <td>-</td>
    <td>45.0</td>
    <td>40.6</td>
    <td>42.5</td>
    <td>26.8</td>
    <td>39.2</td>
    <td>18.8</td>
    <td>4.3</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" ><b>MiniCPM-2.4B</b></td>
    <td>36.87</td>
    <td>53.46</td>
    <td>51.13</td>
    <td>51.07</td>
    <td>50.00</td>
    <td>35.93</td>
    <td>53.83</td>
    <td>10.24</td>
  </tr>
  <tr>
    <td nowrap="nowrap" align="left" ><b>MiniCPM-MoE-8x2B</b></td>
    <td>39.22</td>
    <td>58.90</td>
    <td>58.11</td>
    <td>58.80</td>
    <td>55.49</td>
    <td>41.68</td>
    <td>61.56</td>
    <td>10.52</td>
  </tr>
</tbody>
</table>

</div>

æ³¨ï¼š* è¡¨ç¤ºç»“æœå–è‡ªæŠ€æœ¯æŠ¥å‘Šã€‚&dagger; è¡¨ç¤ºè¯„æµ‹é›†ä¸ºMBPPå…¨é›†ã€‚

#### MiniCPM-S-1B è¯„æµ‹ç»“æœ

- ä»£ç ç”Ÿæˆï¼šåœ¨ HumanEvalï¼ˆ0-shotï¼‰å’Œ MBPPï¼ˆ3-shotï¼‰ä¸Šçš„å¹³å‡ pass@1 å¾—åˆ†ã€‚
- å¸¸è¯†æ¨ç†ï¼šåœ¨ PIQAã€SIQAã€HellaSwagã€WinoGrande å’Œ COPA ä¸Šçš„å¹³å‡ 0-shot å‡†ç¡®ç‡ã€‚
- é˜…è¯»ç†è§£ï¼šåœ¨ BoolQã€LAMBADA å’Œ TyDi QA ä¸Šçš„å¹³å‡ 0-shot å‡†ç¡®ç‡ã€‚

å…¶ä»–æµ‹è¯•é›†ï¼šæˆ‘ä»¬æŠ¥å‘Šåœ¨GSM8Kï¼ˆ8-shotï¼‰ã€MMLUï¼ˆ5-shotï¼‰ã€BBHï¼ˆ3-shotï¼‰å’Œ AGI-Evalï¼ˆ0-shotï¼‰ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ã€‚

|        Setting        | Average<br>Sparsity | Average<br>Performance | Code<br>Generation | Commonsense<br>Reasoning | Reading<br>Comprehension | GSM8K | MMLU  |  BBH  | AGI Eval |
| :-------------------: | :----------------: | :----------------------: | :----------------------: | :---: | :---: | :---: | :---------: | :-----: | :-----------------: |
| LLaMA2-7B    | - | 37.96 | 16.37 | 69.59 | 61.87 | 12.96 | 44.45 | 32.96 | 27.53 |
| ReluLLaMA-7B | 66.98 | 37.62 | 15.85 | 69.64 | 70.54 |  5.84 | 38.64 | 35.07 | 27.73 |
| **ProSparse-7B**\* | 88.11 | 38.31 | 19.47 | 66.29 | 63.33 | 12.74 | 45.21 | 33.59 | 27.55 |
| **ProSparse-7B**   | **89.32** | **38.46** | 19.42 | 66.27 | 63.50 | 12.13 | 45.48 | 34.99 | 27.46 |
| LLaMA2-13B | - | 44.06 | 20.19 | 72.58 | 71.55 | 22.21 | 54.69 | 37.89 | 29.33 |
| ReluLLaMA-13B | 71.56 | 42.74 | 20.19 | 70.44 | 73.29 | 18.50 | 50.58 | 37.97 | 28.22 |
| **ProSparse-13B**\* | 87.97 | **45.07** | 29.03 | 69.75 | 67.54 | 25.40 | 54.78 | 40.20 | 28.76 |
| **ProSparse-13B**   | **88.80** | 44.90 | 28.42 | 69.76 | 66.91 | 26.31 | 54.35 | 39.90 | 28.67 |
| MiniCPM-1B | - | 44.44 | 36.85 | 63.67 | 60.90 | 35.48 | 50.44 | 35.03 | 28.71 |
| **MiniCPM-S-1B**\*  | 86.25 | **44.72** | 41.38 | 64.55 | 60.69 | 34.72 | 49.36 | 34.04 | 28.27 |
| **MiniCPM-S-1B**    | **87.89** | **44.72** | 42.04 | 64.37 | 60.73 | 34.57 | 49.51 | 34.08 | 27.77 |

æ³¨ï¼š
1. ReluLLaMA-7B å’Œ ReluLLaMA-13B çš„ä¸‹è½½é“¾æ¥åˆ†åˆ«æ˜¯ [7B](https://huggingface.co/SparseLLM/ReluLLaMA-7B) and [13B](https://huggingface.co/SparseLLM/ReluLLaMA-13B)ã€‚"ProSparse-7B\*"ã€"ProSparse-13B\*" å’Œ "MiniCPM-S-1B\*" ä»£è¡¨æ²¡æœ‰æ¿€æ´»é˜ˆå€¼åç§»çš„ ProSparse ç‰ˆæœ¬ã€‚
2. å¯¹äº PIQAã€SIQAã€HellaSwagã€WinoGrandeã€COPAã€BoolQã€LAMBADAã€TyDi QA å’Œ AGI-Evalï¼Œæˆ‘ä»¬æ ¹æ®å„ä¸ªé€‰é¡¹çš„ PPL æ¥è¿›è¡Œç­”æ¡ˆé€‰æ‹©ã€‚å¯¹äº GSM8Kã€MMLU å’Œ BBHï¼Œæˆ‘ä»¬ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚

### æ¨¡å‹æ¨ç†

#### HuggingFaceã€vLLMæ¨ç†

å‚è€ƒ MiniCPM 1.0 ä¸­çš„[æ¨¡å‹æ¨ç†](#huggingface-æ¨ç†)éƒ¨åˆ†ã€‚

#### Powerinfer æ¨ç†

é’ˆå¯¹ MiniCPM-S-1B æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Powerinfer è¿›è¡Œæ¨ç†åŠ é€Ÿï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

1. ä¿è¯cmakeç‰ˆæœ¬3.17ä»¥ä¸Šï¼Œå¦‚æœå·²ç»å®‰è£…è¿‡ï¼Œåˆ™è·³è¿‡æ­¤æ­¥éª¤
  ```bash
    # ä¸‹è½½å®‰è£…åŒ…
    sudo wget https://cmake.org/files/v3.23/cmake-3.23.0.tar.gz
    # è§£å‹å®‰è£…åŒ…
    sudo tar -zxvf cmake-3.23.0.tar.gz
    # é…ç½®å®‰è£…ç¯å¢ƒ
    sudo ./configure
    sudo make -j8
    # ç¼–è¯‘å®‰è£…
    sudo make install
    # æŸ¥çœ‹å®‰è£…åç‰ˆæœ¬
    cmake --version
    # è¿”å›ç‰ˆæœ¬å·åˆ™å®‰è£…æˆåŠŸ
    #cmake version 3.23.0
  ```
2. å®‰è£…powerinferï¼š
```bash
  git clone https://github.com/SJTU-IPADS/PowerInfer
  cd PowerInfer
  pip install -r requirements.txt # install Python helpers' dependencies
```
3. cpuç‰ˆæœ¬powerinferç¼–è¯‘,å¦‚æœä½ çš„æœºå™¨åªæœ‰cpuï¼Œæˆ–è€…åªæƒ³ä½¿ç”¨cpuè¿›è¡Œæ¨ç†ï¼Œåˆ™è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
  cmake -S . -B build
  cmake --build build --config Release
```
4. gpuç‰ˆæœ¬powerinferç¼–è¯‘,å¦‚æœä½ çš„æœºå™¨æœ‰gpuï¼Œåˆ™å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```bash
  cmake -S . -B build -DLLAMA_CUBLAS=ON
  cmake --build build --config Release
```
5. è·å–ç¨€ç–æ¨¡å‹
```bash
git clone https://huggingface.co/openbmb/MiniCPM-S-1B-sft-gguf/tree/main
#or
git clone https://modelscope.cn/models/OpenBMB/MiniCPM-S-1B-sft-gguf
```
6. æ¨¡å‹æ¨ç†ï¼š
```bash
cd PowerInfer
# ä»¥ä¸‹æ˜¯å‘½ä»¤æ¨¡ç‰ˆï¼Œoutput_token_countä¸ºæœ€å¤§è¾“å‡ºtokensï¼Œthread_num ä¸ºçº¿ç¨‹æ•°ï¼Œpromptä¸ºè¾“å…¥promptå­—ç¬¦
#./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt
# ä»¥ä¸‹æ˜¯ç¤ºä¾‹
./build/bin/main -m /root/ld/ld_model_pretrain/1b-s-minicpm/MiniCPM-S-1B-sft.gguf -n 2048 -t 8 -p '<ç”¨æˆ·>hello,tell me a story please.<AI>'
```
</details>

## MiniCPM 1.0

<details>
<summary>æŸ¥çœ‹ MiniCPM 1.0 çš„è¯¦ç»†ä¿¡æ¯</summary>

MiniCPM-2B è¯­è¨€æ¨¡å‹æœ‰ 24äº¿ï¼ˆ2.4Bï¼‰çš„éè¯åµŒå…¥å‚æ•°é‡, æ€»è®¡ 2.7B å‚æ•°é‡ã€‚
- ç»è¿‡ SFT åï¼ŒMiniCPM-2B åœ¨å…¬å¼€è¯„æµ‹é›†ä¸Šä¸ Mistral-7B è¡¨ç°ç›¸è¿‘ï¼ˆä¸­æ–‡ã€æ•°å­¦ã€ä»£ç èƒ½åŠ›æ›´ä¼˜ï¼‰ï¼Œæ•´ä½“æ€§èƒ½è¶…è¶Š Llama2-13Bã€MPT-30Bã€Falcon-40B ç­‰æ¨¡å‹ã€‚
- ç»è¿‡ DPO åï¼ŒMiniCPM-2B åœ¨ MTBench ä¸Šä¹Ÿè¶…è¶Šäº† Llama2-70B-Chatã€Vicuna-33Bã€Mistral-7B-Instruct-v0.1ã€Zephyr-7B-alpha ç­‰ä¼—å¤šä»£è¡¨æ€§å¼€æºå¤§æ¨¡å‹ã€‚

æ³¨æ„ï¼šä¸ºäº†ä¿è¯åœ¨å­¦æœ¯ç ”ç©¶ç”¨é€”ä¸Šæ¨¡å‹çš„é€šç”¨æ€§ï¼Œæˆ‘ä»¬**æœªå¯¹ MiniCPM-2B è¿›è¡Œä»»ä½•èº«ä»½è®¤åŒè®­ç»ƒ**ã€‚åŒæ—¶ç”±äºæˆ‘ä»¬ç”¨ ShareGPT å¼€æºè¯­æ–™ä½œä¸ºéƒ¨åˆ†è®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹å¯èƒ½ä¼šè¾“å‡ºç±»ä¼¼ GPT ç³»åˆ—æ¨¡å‹çš„èº«ä»½è®¤åŒä¿¡æ¯ã€‚

### è¯„æµ‹ç»“æœ

#### è¯„æµ‹è®¾ç½®

* ç”±äºå¤§æ¨¡å‹è¯„æµ‹éš¾ä»¥ç»Ÿä¸€ï¼Œä¸”å¤§é‡è¯„æµ‹ä¹Ÿæ²¡æœ‰å…¬å¼€çš„promptå’Œæµ‹è¯•ä»£ç ï¼Œå¯¹äºå…·ä½“è¯„æµ‹æ–¹å¼ï¼Œæˆ‘ä»¬åªèƒ½å°½é‡åšåˆ°é€‚åˆå„ç±»æ¨¡å‹ã€‚
* æ•´ä½“è€Œè¨€ï¼Œæˆ‘ä»¬æµ‹è¯•æ—¶é‡‡ç”¨ç»Ÿä¸€çš„promptè¾“å…¥ï¼Œå¹¶æŒ‰ç…§å„æ¨¡å‹å¯¹åº”çš„æ¨¡æ¿è¿›è¡Œè¾“å…¥è°ƒæ•´ã€‚
* **è¯„æµ‹è„šæœ¬åŠpromptå·²å¼€æºåœ¨æˆ‘ä»¬çš„Githubä»“åº“ä¸­ï¼Œä¹Ÿæ¬¢è¿æ›´å¤šå¼€å‘è€…æ¥ä¸æ–­æ”¹è¿›æˆ‘ä»¬çš„è¯„æµ‹æ–¹å¼ã€‚**
  * æ–‡æœ¬è¯„æµ‹éƒ¨åˆ†ï¼Œé‡‡ç”¨äº†æˆ‘ä»¬çš„å¼€æºå¤§æ¨¡å‹èƒ½åŠ›è¯„æµ‹æ¡†æ¶[UltraEval](https://github.com/OpenBMB/UltraEval)ã€‚ä»¥ä¸‹ä¸ºå¼€æºæ¨¡å‹å¤ç°æµç¨‹ï¼š
    * å®‰è£…UltraEval
      ```shell
      git clone https://github.com/OpenBMB/UltraEval.git
      cd UltraEval
      pip install -e .
      ```
    * ä¸‹è½½ç›¸å…³æ•°æ®å¹¶è§£å‹å¤„ç†
      ```shell
      wget -O RawData.zip "https://cloud.tsinghua.edu.cn/f/71b5232264ae4833a4d0/?dl=1"
      unzip RawData.zip
      python data_process.py
      ```
    * æ‰§è¡Œè¯„æµ‹è„šæœ¬(æä¾›äº†æ¨¡æ¿ï¼Œå¯è‡ªå®šä¹‰)
      ```shell
      bash run_eval.sh
      ```

#### éƒ¨ç½²æ¨¡å¼

* å› ä¸ºMiniCPMé‡‡ç”¨Mupçš„ç»“æ„ï¼Œä¸ç°æœ‰æ¨¡å‹åœ¨å…·ä½“è®¡ç®—ä¸Šæœ‰ç»†å¾®å·®åˆ«ï¼Œæˆ‘ä»¬æ˜¯åŸºäºvllm=0.2.2ç‰ˆæœ¬è¿›è¡Œäº†æˆ‘ä»¬æ¨¡å‹çš„å®ç°ã€‚
* **å¯¹äºéMiniCPMæ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†vllm=0.2.7çš„æœ€æ–°ç‰ˆæœ¬è¿›è¡Œæ¨ç†ã€‚**

#### è¯„æµ‹åº¦é‡

* å¯¹äºQAä»»åŠ¡ï¼ˆé€‰æ‹©é¢˜ä»»åŠ¡ï¼‰ï¼Œæˆ‘ä»¬é€‰ç”¨ä¸¤ç§æ–¹å¼è¿›è¡Œæµ‹è¯•ï¼š
  * PPLï¼šå°†é€‰é¡¹ä½œä¸ºé¢˜ç›®ç”Ÿæˆçš„å»¶ç»­ï¼Œå¹¶æ ¹æ®å„ä¸ªé€‰é¡¹çš„PPLæ¥è¿›è¡Œç­”æ¡ˆé€‰æ‹©ï¼›
  * ç¬¬äºŒç§æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆé€‰é¡¹ã€‚
* å¯¹äºä¸åŒæ¨¡å‹ï¼Œè¿™ä¸¤ç§æ–¹å¼å¾—åˆ°çš„ç»“æœå·®å¼‚è¾ƒå¤§ã€‚MiniCPMä¸¤ç§æ¨¡å¼ä¸Šçš„ç»“æœè¾ƒä¸ºæ¥è¿‘ï¼Œè€ŒMistral-7B-v0.1ç­‰æ¨¡å‹åœ¨PPLä¸Šè¡¨ç°è¾ƒå¥½ï¼Œç›´æ¥ç”Ÿæˆä¸Šæ•ˆæœè¾ƒå·®ã€‚
* åœ¨å…·ä½“è¯„æµ‹æ—¶ï¼Œæˆ‘ä»¬ä»¥ä¸¤ç§è¯„æµ‹æ–¹å¼å¾—åˆ†çš„æœ€é«˜è€…ä¸ºæœ€ç»ˆç»“æœï¼Œä»¥æ­¤ä¿è¯å¯¹æ¯”çš„å…¬å¹³æ€§(ä»¥ä¸‹è¡¨æ ¼ä¸­*å·è¡¨ç¤ºé‡‡ç”¨PPL)ã€‚

#### æ–‡æœ¬æ¨¡å‹è¯„æµ‹

**è¶Šçº§æ¯”è¾ƒ:**
|æ¨¡å‹|å¹³å‡åˆ†|è‹±æ–‡å‡åˆ†|ä¸­æ–‡å‡åˆ†|C-Eval|CMMLU|MMLU|HumanEval|MBPP|GSM8K|MATH|BBH|ARC-E|ARC-C|HellaSwag|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|Llama2-7B|35.40|36.21|31.765|32.42|31.11|44.32|12.2|27.17|13.57|1.8|33.23|75.25|42.75|75.62*|
|Qwen-7B|49.46|47.19|59.655|58.96|60.35|57.65|17.07|42.15|41.24|5.34|37.75|83.42|64.76|75.32*|
|Deepseek-7B|39.96|39.15|43.64|42.82|44.45|47.82|20.12|41.45|15.85|1.53|33.38|74.58*|42.15*|75.45*|
|Mistral-7B|48.97|49.96|44.54|46.12|42.96|62.69|27.44|45.2|33.13|5.0|41.06|83.92|70.73|80.43*|
|Llama2-13B|41.48|42.44|37.19|37.32|37.06|54.71|17.07|32.55|21.15|2.25|37.92|78.87*|58.19|79.23*|
|MPT-30B|38.17|39.82|30.72|29.34|32.09|46.56|21.95|35.36|10.31|1.56|38.22|78.66*|46.08*|79.72*|
|Falcon-40B|43.62|44.21|40.93|40.29|41.57|53.53|24.39|36.53|22.44|1.92|36.24|81.94*|57.68|83.26*|
|MiniCPM-2B|52.33|52.6|51.1|51.13|51.07|53.46|50.00|47.31|53.83|10.24|36.87|85.44|68.00|68.25|

**åŒçº§æ¯”è¾ƒï¼š**
|æ¨¡å‹|å¹³å‡åˆ†|è‹±æ–‡å‡åˆ†|ä¸­æ–‡å‡åˆ†|C-Eval|CMMLU|MMLU|HumanEval|MBPP|GSM8K|MATH|BBH|ARC-E|ARC-C|HellaSwag|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|TinyLlama-1.1B|25.36|25.55|24.525|25.02|24.03|24.3|6.71|19.91|2.27|0.74|28.78|60.77*|28.15*|58.33*|Qwen-1.8B|34.72|31.87|47.565|49.81|45.32|43.37|7.93|17.8|19.26|2.42|29.07|63.97*|43.69|59.28*|
|Qwen-1.8B|34.72|31.87|47.57|49.81|45.32|43.37|7.93|17.80|19.26|2.42|29.07|63.97*|43.69|59.28*|
|Gemini Nano-3B|-|-|-|-|-|-|-|27.2(report)|22.8(report)|-|42.4(report)|-|-|-|
|StableLM-Zephyr-3B|43.46|46.31|30.62|30.34|30.89|45.9|35.37|31.85|52.54|12.49|37.68|73.78|55.38|71.87*|
|Phi-2-2B|48.84|54.41|23.78|23.37|24.18|52.66|47.56|55.04|57.16|3.5|43.39|86.11|71.25|73.07*|
|MiniCPM-2B|52.33|52.6|51.10|51.13|51.07|53.46|50.00|47.31|53.83|10.24|36.87|85.44|68.00|68.25|

**Chatæ¨¡å‹æ¯”è¾ƒï¼š**
|æ¨¡å‹|å¹³å‡åˆ†|è‹±æ–‡å‡åˆ†|ä¸­æ–‡å‡åˆ†|C-Eval|CMMLU|MMLU|HumanEval|MBPP|GSM8K|MATH|BBH|ARC-E|ARC-C|HellaSwag|
|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|
|ChatGLM2-6B|37.98|35.17|50.63|52.05|49.21|45.77|10.37|9.38|22.74|5.96|32.6|74.45|56.82|58.48*|
|Mistral-7B-Instruct-v0.1|44.36|45.89|37.51|38.06|36.96|53.56|29.27|39.34|28.73|3.48|39.52|81.61|63.99|73.47*|
|Mistral-7B-Instruct-v0.2|50.91|52.83|42.235|42.55|41.92|60.51|36.59|48.95|40.49|4.95|39.81|86.28|73.38|84.55*|
|Qwen-7B-Chat|44.93|42.05|57.9|58.57|57.23|56.03|15.85|40.52|42.23|8.3|37.34|64.44*|39.25*|74.52*|
|Yi-6B-Chat|50.46|45.89|70.995|70.88|71.11|62.95|14.02|28.34|36.54|3.88|37.43|84.89|70.39|74.6*|
|Baichuan2-7B-Chat|44.68|42.74|53.39|53.28|53.5|53|21.34|32.32|25.25|6.32|37.46|79.63|60.15|69.23*|
|Deepseek-7B-chat|49.34|49.56|48.335|46.95|49.72|51.67|40.85|48.48|48.52|4.26|35.7|76.85|63.05|76.68*|
|Llama2-7B-Chat|38.16|39.17|33.59|34.54|32.64|47.64|14.02|27.4|21.15|2.08|35.54|74.28|54.78|75.65*|
|MiniCPM-2B|52.33|52.6|51.10|51.13|51.07|53.46|50.00|47.31|53.83|10.24|36.87|85.44|68.00|68.25|

**DPOåæ¨¡å‹æ¯”è¾ƒï¼š**

|æ¨¡å‹|MT-bench|
|---|---|
|GPT-4-turbo|9.32|
|GPT-3.5-turbo|8.39|
|Mistral-8*7b-Instruct-v0.1|8.30|
|Claude-2.1|8.18|
|Zephyr-7B-beta|7.34|
|**MiniCPM-2B**|**7.25**|
|Vicuna-33B|7.12|
|Zephyr-7B-alpha|6.88|
|LLaMA-2-70B-chat|6.86|
|Mistral-7B-Instruct-v0.1|6.84|
|MPT-34B-instruct|6.39|


### å¿«é€Ÿä¸Šæ‰‹ 

#### åœ¨çº¿ä½“éªŒ

- [Colab](https://colab.research.google.com/drive/1tJcfPyWGWA5HezO7GKLeyeIso0HyOc0l?usp=sharing)

#### åŸºäºGradioçš„ç½‘é¡µç‰ˆDemo

* ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¯åŠ¨åŸºäºGradioçš„ç½‘é¡µç‰ˆdemoï¼š

```shell
# generation powered by vllm
python demo/minicpm/vllm_based_demo.py --model_path <vllmcpm_repo_path>
# generation powered by huggingface
python demo/minicpm/hf_based_demo.py --model_path <hf_repo_path>
```

#### HuggingFace æ¨ç†

##### MiniCPM-2B

å®‰è£…`transformers>=4.36.0`ä»¥åŠ`accelerate`åï¼Œè¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
torch.manual_seed(0)

path = 'openbmb/MiniCPM-2B-dpo-bf16'
tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)

responds, history = model.chat(tokenizer, "å±±ä¸œçœæœ€é«˜çš„å±±æ˜¯å“ªåº§å±±, å®ƒæ¯”é»„å±±é«˜è¿˜æ˜¯çŸ®ï¼Ÿå·®è·å¤šå°‘ï¼Ÿ", temperature=0.5, top_p=0.8, repetition_penalty=1.02)
print(responds)
```

##### MiniCPM-2B ï¼ˆLlama Formatï¼‰

æˆ‘ä»¬å°†MiniCPMçš„æ¨¡å‹æƒé‡è½¬åŒ–æˆäº†Llamaä»£ç å¯ä»¥ç›´æ¥è°ƒç”¨çš„[æ ¼å¼](https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16-llama-format)ï¼Œä»¥ä¾¿å¤§å®¶å°è¯•:

```python
import torch
from transformers import LlamaTokenizerFast, LlamaForCausalLM
model_path = "openbmb/MiniCPM-2B-dpo-bf16-llama-format"
tokenizer = LlamaTokenizerFast.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map='cuda', trust_remote_code=True)

prompt="Now you act like a terminal situated within a beginner's C++ practice repository folder, please provide the output for the command: `ls -l`"
input_ids = tokenizer.encode("<ç”¨æˆ·>{}<AI>".format(prompt), return_tensors='pt', add_special_tokens=True).cuda()
responds = model.generate(input_ids, temperature=0.3, top_p=0.8, repetition_penalty=1.02, max_length=1024)
responds = tokenizer.decode(responds[0], skip_special_tokens=True)
print(responds)
```

#### vLLM æ¨ç†

å®‰è£… [vLLM](https://github.com/vllm-project/vllm)ã€‚

```shell
pip install "vllm>=0.4.1"
```

å…·ä½“æ¨ç†ä»£ç è§[è¿™é‡Œ](#vllm)ã€‚

#### SGLang æ¨ç†

å®‰è£… [SGLang](https://github.com/sgl-project/sglang)ã€‚

* é¦–å…ˆéœ€è¦å¯åŠ¨ä¸€ä¸ªæœåŠ¡:

```bash
python -m sglang.launch_server --model-path openbmb/MiniCPM-2B-dpo-fp16 --trust-remote-code --port 30000
```

* ä¸‹é¢æ˜¯ä¸€ä¸ªæ¨ç†ä»£ç çš„æ ·ä¾‹:

```python
from sglang import function, gen, set_default_backend, RuntimeEndpoint

@function
def text_qa(s, question):
    s += "<ç”¨æˆ·>" + question + "<AI>"
    s += gen("answer", max_tokens=1024, temperature=0.7, top_p=0.7)

set_default_backend(RuntimeEndpoint("http://localhost:30000"))

state = text_qa.run(
    question="What is the capital of China?",
)

print(state["answer"])
```

#### llama.cppã€Ollamaã€fastllmã€mlx_lmæ¨ç†
MiniCPMæ”¯æŒ[llama.cpp](https://github.com/ggerganov/llama.cpp/) ã€[ollama](https://github.com/ollama/ollama)ã€[fastllm](https://github.com/ztxz16/fastllm)ã€[mlx_lm](https://github.com/ml-explore/mlx-examples)æ¨ç†ã€‚æ„Ÿè°¢[@runfuture](https://github.com/runfuture)å¯¹llama.cppå’Œollamaçš„é€‚é…ã€‚

è¯·å‚è€ƒ MiniCPM çŸ¥è¯†åº“ä¸­çš„[è¾¹ç«¯éƒ¨ç½²æ•™ç¨‹](https://modelbest.feishu.cn/wiki/VL5kw9DsEiRDmJkEyTUcydE0nie)ã€‚

#### æ¨¡å‹é‡åŒ–

è¯·å‚è€ƒ MiniCPM çŸ¥è¯†åº“ä¸­çš„[é‡åŒ–æŒ‡å—](https://modelbest.feishu.cn/wiki/EatbwdLuvitbbMk2X5wcX6h5n7c)ã€‚

#### æ¨¡å‹å¾®è°ƒ

- ä¸€å¼  1080/2080 å¯å®ç°é«˜æ•ˆå‚æ•°å¾®è°ƒï¼š[ä»£ç ](https://github.com/OpenBMB/MiniCPM/tree/main/finetune)
- mlx å¾®è°ƒï¼š[æ•™ç¨‹](https://modelbest.feishu.cn/wiki/AIU3wbREcirOm9kkvd7cxujFnMb#share-ASrDdvFAloHtycxfy85cLNhAnd3)
- [xtuner](https://github.com/InternLM/xtuner): [MiniCPMé«˜æ•ˆç‡å¾®è°ƒçš„ä¸äºŒé€‰æ‹©](https://modelbest.feishu.cn/wiki/AIU3wbREcirOm9kkvd7cxujFnMb#AMdXdzz8qoadZhxU4EucELWznzd)
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory.git)ï¼š[MiniCPMå¾®è°ƒä¸€é”®å¼è§£å†³æ–¹æ¡ˆ](https://modelbest.feishu.cn/wiki/AIU3wbREcirOm9kkvd7cxujFnMb#BAWrdSjXuoFvX4xuIuzc8Amln5E)

</details>


## å¼€æºåè®®

#### æ¨¡å‹åè®®

* æœ¬ä»“åº“ä¸­ä»£ç ä¸ MiniCPM æ¨¡å‹æƒé‡ä¾ç…§ [Apache-2.0](https://github.com/OpenBMB/MiniCPM/blob/main/LICENSE) åè®®å¼€æº

#### å£°æ˜

* ä½œä¸ºä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼ŒMiniCPM é€šè¿‡å­¦ä¹ å¤§é‡çš„æ–‡æœ¬æ¥ç”Ÿæˆå†…å®¹ï¼Œä½†å®ƒæ— æ³•ç†è§£ã€è¡¨è¾¾ä¸ªäººè§‚ç‚¹æˆ–ä»·å€¼åˆ¤æ–­ï¼Œå®ƒæ‰€è¾“å‡ºçš„ä»»ä½•å†…å®¹éƒ½ä¸ä»£è¡¨æ¨¡å‹å¼€å‘è€…çš„è§‚ç‚¹å’Œç«‹åœºã€‚
* å› æ­¤ç”¨æˆ·åœ¨ä½¿ç”¨ MiniCPM ç”Ÿæˆçš„å†…å®¹æ—¶ï¼Œåº”è‡ªè¡Œè´Ÿè´£å¯¹å…¶è¿›è¡Œè¯„ä¼°å’ŒéªŒè¯ã€‚
* å¦‚æœç”±äºä½¿ç”¨ MiniCPM å¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

## å¼€å‘æœºæ„

æœ¬é¡¹ç›®ç”±ä»¥ä¸‹æœºæ„å…±åŒå¼€å‘ï¼š

- <img src="assets/modelbest.png" width="28px"> [é¢å£æ™ºèƒ½](https://modelbest.cn/)
- <img src="assets/thunlp.png" width="28px"> [æ¸…åå¤§å­¦è‡ªç„¶è¯­è¨€å¤„ç†å®éªŒå®¤](https://nlp.csai.tsinghua.edu.cn/)
- <img src="assets/RUC.png" width="28px"> [äººå¤§é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢](https://linyankai.github.io/)

## å·¥ä½œå¼•ç”¨

* å¦‚æœè§‰å¾— MiniCPM æœ‰åŠ©äºæ‚¨çš„å·¥ä½œï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š[MiniCPM1](https://arxiv.org/abs/2404.06395)ï¼Œ[MiniCPM4](https://github.com/OpenBMB/MiniCPM/blob/main/report/MiniCPM_4_Technical_Report.pdf)

```
@article{minicpm4,
  title={MiniCPM4: Ultra-Efficient LLMs on End Devices},
  author={MiniCPM Team},
  year={2025}
}

@inproceedings{huminicpm,
  title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and Cui, Ganqu and He, Chaoqun and Zhao, Weilin and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and others},
  booktitle={First Conference on Language Modeling},
  year={2024}
}
```
